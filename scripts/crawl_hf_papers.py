#!/usr/bin/env python3
"""
Hugging Face Daily Papers í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
GitHub Actionsì—ì„œ ì‹¤í–‰í•˜ì—¬ ì¼ì¼ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ê³  Jekyll í¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
ì¼ê°„ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥í•˜ê³ , ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬í•˜ë©°, Abstractë„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import re
import json
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from collections import defaultdict
import time
try:
    import feedparser
except ImportError:
    feedparser = None


class HFDailyPapersCrawler:
    """Hugging Face Daily Papers í¬ë¡¤ëŸ¬"""
    
    def __init__(self, posts_dir: str = "_posts", data_dir: str = "_data/papers"):
        self.posts_dir = Path(posts_dir)
        self.posts_dir.mkdir(exist_ok=True)
        
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Hugging Face Daily Papers ê´€ë ¨ URLë“¤
        self.base_url = "https://huggingface.co"
        self.papers_url = "https://huggingface.co/papers"
        # ì—¬ëŸ¬ RSS í”¼ë“œ URL ì‹œë„
        self.rss_urls = [
            "https://huggingface.co/blog/tags/papers/rss.xml",
            "https://huggingface.co/blog/rss.xml",  # ì „ì²´ ë¸”ë¡œê·¸ RSS
        ]
        self.rss_url = self.rss_urls[0]  # ê¸°ë³¸ê°’
    
    def fetch_daily_papers(self, target_date: Optional[datetime] = None) -> List[Dict]:
        """
        íŠ¹ì • ë‚ ì§œì˜ ë…¼ë¬¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì¢‹ì•„ìš” ìˆ˜ í¬í•¨).
        
        Args:
            target_date: í¬ë¡¤ë§í•  ë‚ ì§œ (Noneì´ë©´ ì˜¤ëŠ˜)
            
        Returns:
            ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ (ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬ë¨)
        """
        if target_date is None:
            target_date = datetime.utcnow()
        
        papers = []
        
        # RSS í”¼ë“œì—ì„œ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°
        if feedparser:
            try:
                papers.extend(self._fetch_from_rss(target_date))
            except Exception:
                pass
        
        # ì›¹ í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°
        try:
            papers.extend(self._fetch_daily_from_web(target_date))
        except Exception:
            pass
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_papers = []
        for paper in papers:
            url = paper.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_papers.append(paper)
        
        # ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
        unique_papers.sort(key=lambda x: x.get('likes', 0), reverse=True)
        
        # ìƒìœ„ 10ê°œë§Œ ì„ íƒ
        top_papers = unique_papers[:10]
        print(f"\n[ì„ íƒ] ì´ {len(unique_papers)}ê°œ ì¤‘ ìƒìœ„ 10ê°œ ì„ íƒ")
        
        # ìƒìœ„ 10ê°œë§Œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (Abstract í¬í•¨)
        enriched_papers = []
        for i, paper in enumerate(top_papers, 1):
            try:
                print(f"  [{i}/10] ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {paper.get('title', 'Unknown')[:50]}...")
                enriched = self._enrich_paper_details(paper)
                enriched_papers.append(enriched)
                # ìš”ì²­ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(1)
            except Exception as e:
                print(f"  ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({paper.get('url', 'unknown')}): {e}")
                enriched_papers.append(paper)
        
        # ìµœì¢… ì •ë ¬ (ì¢‹ì•„ìš” ìˆ˜ë¡œ)
        enriched_papers.sort(key=lambda x: x.get('likes', 0), reverse=True)
        
        print(f"\n[ì™„ë£Œ] ìµœì¢… {len(enriched_papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return enriched_papers
    
    def _fetch_from_rss(self, target_date: datetime) -> List[Dict]:
        """RSS í”¼ë“œì—ì„œ íŠ¹ì • ë‚ ì§œì˜ ë…¼ë¬¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if feedparser is None:
            return []
        
        papers = []
        feed = None
        
        # RSS URL ì‹œë„
        for rss_url in self.rss_urls:
            try:
                feed = feedparser.parse(rss_url)
                if feed.entries:
                    print(f"RSS í”¼ë“œ: {len(feed.entries)}ê°œ í•­ëª©")
                    break
            except Exception:
                continue
        
        if not feed or not feed.entries:
            return papers
        
        target_date_str = target_date.strftime('%Y-%m-%d')
        yesterday = (target_date - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # ë‚ ì§œ ë§¤ì¹­
        for entry in feed.entries[:30]:
            entry_date = entry.get('published', '')
            if (target_date_str in entry_date or yesterday in entry_date or
                self._is_same_date(entry_date, target_date) or
                self._is_same_date(entry_date, target_date - timedelta(days=1))):
                papers.append({
                    'title': entry.get('title', ''),
                    'url': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'summary': entry.get('summary', ''),
                    'likes': 0
                })
        
        # ë‚ ì§œ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ìµœì‹  10ê°œ ì‚¬ìš©
        if not papers:
            for entry in feed.entries[:10]:
                papers.append({
                    'title': entry.get('title', ''),
                    'url': entry.get('link', ''),
                    'published': entry.get('published', datetime.utcnow().isoformat()),
                    'summary': entry.get('summary', ''),
                    'likes': 0
                })
        
        return papers
    
    def _fetch_daily_from_web(self, target_date: datetime) -> List[Dict]:
        """ì›¹ í˜ì´ì§€ì—ì„œ ì¼ì¼ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        papers = []
        
        # ì˜¬ë°”ë¥¸ URL í˜•ì‹: https://huggingface.co/papers/date/YYYY-MM-DD
        # ì‹¤ì œ ì˜ˆì‹œ: https://huggingface.co/papers/date/2025-10-31
        date_str = target_date.strftime('%Y-%m-%d')
        url = f"{self.papers_url}/date/{date_str}"
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(url, timeout=30, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë…¼ë¬¸ í•­ëª© ì°¾ê¸° (h3 íƒœê·¸ë¡œ ì œëª© ì°¾ê¸°)
            # êµ¬ì¡°: h3 > a (ì œëª© ë§í¬), ì¢‹ì•„ìš” ìˆ˜ëŠ” ê°™ì€ ë ˆë²¨ì— ìˆìŒ
            paper_headings = soup.find_all('h3')
            
            
            for heading in paper_headings:
                try:
                    # ì œëª© ë§í¬ ì°¾ê¸°
                    title_link = heading.find('a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text(strip=True)
                    if not title:
                        continue
                    
                    paper_url = title_link.get('href', '')
                    
                    if not paper_url.startswith('http'):
                        if paper_url.startswith('/'):
                            paper_url = self.base_url + paper_url
                        else:
                            paper_url = f"{self.base_url}/papers/{paper_url}"
                    
                    # ì¢‹ì•„ìš” ìˆ˜ ì°¾ê¸° - h3ì˜ ë¶€ëª¨ ìš”ì†Œì—ì„œ ì°¾ê¸°
                    likes = 0
                    parent = heading.parent
                    if parent:
                        # ë¶€ëª¨ ìš”ì†Œì˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì°¾ê¸°
                        parent_text = parent.get_text()
                        # ì¢‹ì•„ìš” ìˆ˜ëŠ” ë³´í†µ í° ìˆ«ìì´ê³ , h3 ê·¼ì²˜ì— ìˆìŒ
                        # í˜•ì‹: "97\n" ë˜ëŠ” " 97 " ê°™ì€ íŒ¨í„´
                        numbers = re.findall(r'\b(\d+)\b', parent_text)
                        
                        # h3 ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ìˆ«ìê°€ ì¢‹ì•„ìš” ìˆ˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                        # ë˜ëŠ” h3 ì•ì— ìˆëŠ” í° ìˆ«ì
                        if numbers:
                            # ì²« ë²ˆì§¸ í° ìˆ«ì (10 ì´ìƒ)ë¥¼ ì¢‹ì•„ìš” ìˆ˜ë¡œ ì¶”ì •
                            for num_str in numbers:
                                try:
                                    num = int(num_str)
                                    if num >= 10:  # ì¢‹ì•„ìš”ëŠ” ë³´í†µ 10 ì´ìƒ
                                        likes = num
                                        break
                                except (ValueError, TypeError):
                                    pass
                        
                        # h3ì˜ ë‹¤ìŒ í˜•ì œ ìš”ì†Œ í™•ì¸
                        if likes == 0:
                            current = heading.next_sibling
                            checked = 0
                            while current and checked < 3:
                                if hasattr(current, 'get_text'):
                                    text = current.get_text(strip=True)
                                    like_match = re.search(r'^(\d+)$', text)
                                    if like_match:
                                        num = int(like_match.group(1))
                                        if num >= 10:
                                            likes = num
                                            break
                                current = getattr(current, 'next_sibling', None)
                                checked += 1
                    
                    if title and paper_url:
                        papers.append({
                            'title': title,
                            'url': paper_url,
                            'published': target_date.isoformat(),
                            'likes': likes
                        })
                except Exception:
                    continue
                    
        except Exception:
            pass
        
        return papers
    
    def _enrich_paper_details(self, paper: Dict) -> Dict:
        """ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (Abstract í¬í•¨)"""
        url = paper.get('url', '')
        if not url:
            return paper
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, timeout=30, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Abstract/Description ì¶”ì¶œ
            abstract = paper.get('abstract', '')
            
            # Abstract ì¶”ì¶œ - ë” êµ¬ì²´ì ì¸ ì„ íƒì ì‹œë„
            abstract_candidates = []
            
            # 1. ëª…ì‹œì ì¸ abstract/summary ì„¹ì…˜ ì°¾ê¸°
            for selector in [
                'div[class*="abstract"]',
                'div[class*="summary"]',
                'section[class*="abstract"]',
                'section[class*="summary"]',
                'p[class*="abstract"]',
                'p[class*="summary"]',
            ]:
                for elem in soup.select(selector):
                    text = elem.get_text(separator=' ', strip=True)
                    if text and 100 <= len(text) <= 2000:
                        abstract_candidates.append(text)
            
            # 2. main/articleì—ì„œ ì²« ë²ˆì§¸ ê¸´ ë¬¸ë‹¨ ì°¾ê¸°
            for container in soup.find_all(['main', 'article']):
                paragraphs = container.find_all(['p', 'div'], limit=10)
                for p in paragraphs:
                    text = p.get_text(separator=' ', strip=True)
                    # AbstractëŠ” ë³´í†µ ê¸´ ë¬¸ë‹¨ì´ê³ , ë²„íŠ¼/ë§í¬ í…ìŠ¤íŠ¸ê°€ ì•„ë‹˜
                    if (text and 150 <= len(text) <= 2000 and
                        'Join the discussion' not in text and
                        'Subscribe' not in text and
                        'Get trending papers' not in text and
                        'on this paper page' not in text and
                        not text.startswith('http') and
                        len(text.split()) >= 20):  # ìµœì†Œ 20ë‹¨ì–´
                        abstract_candidates.append(text)
                        break  # ì²« ë²ˆì§¸ ê¸´ ë¬¸ë‹¨ë§Œ ì‚¬ìš©
            
            # 3. ê°€ì¥ ì í•©í•œ í›„ë³´ ì„ íƒ
            if abstract_candidates:
                # í•„í„°ë§: ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                filtered = []
                unwanted_phrases = [
                    'Join the discussion',
                    'on this paper page',
                    'Subscribe',
                    'Get trending papers',
                    'View on',
                    'Download',
                    'Like',
                    'Share',
                ]
                
                for candidate in abstract_candidates:
                    # ê° ë¶ˆí•„ìš”í•œ êµ¬ë¬¸ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì²´í¬
                    has_unwanted = any(phrase.lower() in candidate.lower() for phrase in unwanted_phrases)
                    if not has_unwanted and len(candidate) >= 100:
                        filtered.append(candidate)
                
                if filtered:
                    # ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ì„ íƒ (ë” ìì„¸í•œ Abstractì¼ ê°€ëŠ¥ì„±)
                    abstract = max(filtered, key=len)
                elif abstract_candidates:
                    # í•„í„°ë§ í›„ë³´ê°€ ì—†ìœ¼ë©´ ì›ë³¸ í›„ë³´ ì¤‘ì—ì„œ ê°€ì¥ ê¸´ ê²ƒ ì„ íƒ
                    abstract = max(abstract_candidates, key=len)
                    # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
                    for unwanted in unwanted_phrases:
                        if unwanted.lower() in abstract.lower():
                            parts = re.split(re.escape(unwanted), abstract, flags=re.I)
                            if parts:
                                abstract = parts[0].strip()
                                break
            
            # 4. ë©”íƒ€ íƒœê·¸ ì‹œë„ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
            if not abstract or len(abstract) < 50:
                for meta in soup.find_all('meta', attrs={'name': ['description'], 'property': ['og:description']}):
                    desc = meta.get('content', '').strip()
                    if (desc and len(desc) >= 100 and
                        'Join the discussion' not in desc and
                        'on this paper page' not in desc):
                        abstract = desc
                        break
            
            # 5. ìµœì¢… ì •ë¦¬
            if abstract:
                # ë¶ˆí•„ìš”í•œ êµ¬ë¬¸ ì œê±°
                unwanted_patterns = [
                    r'Join the discussion.*?$',
                    r'on this paper page.*?$',
                    r'Subscribe.*?$',
                    r'Get trending papers.*?$',
                ]
                for pattern in unwanted_patterns:
                    abstract = re.sub(pattern, '', abstract, flags=re.I).strip()
                
                # ë„ˆë¬´ ì§§ê±°ë‚˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                if (len(abstract) < 50 or
                    abstract.lower() in ['join the discussion', 'subscribe', 'get trending papers'] or
                    'on this paper page' in abstract.lower()):
                    abstract = ''
            
            # ì¢‹ì•„ìš” ìˆ˜, ë…¼ë¬¸ ë§í¬, ì½”ë“œ ë§í¬, íƒœê·¸ ì¶”ì¶œ
            likes = paper.get('likes', 0)
            for elem in soup.find_all(['span', 'div', 'button'], class_=re.compile(r'like|favorite', re.I)):
                match = re.search(r'(\d+)', elem.get_text(strip=True))
                if match:
                    likes = max(likes, int(match.group(1)))
            
            title = paper.get('title', '')
            if not title:
                h1 = soup.find('h1') or soup.find('title')
                if h1:
                    title = h1.get_text(strip=True)
            
            paper_link = paper.get('paper_link', '')
            for link in soup.find_all('a', href=re.compile(r'(arxiv|pdf|doi)', re.I)):
                paper_link = link.get('href', '')
                break
            
            code_link = paper.get('code_link', '')
            for link in soup.find_all('a', href=re.compile(r'(github|gitlab)', re.I)):
                code_link = link.get('href', '')
                break
            
            tags = paper.get('tags', [])
            if not tags:
                tags = [tag.get_text(strip=True) for tag in soup.find_all(['a', 'span'], class_=re.compile(r'tag', re.I))[:10]]
            
            # ì—…ë°ì´íŠ¸ëœ ì •ë³´ ë°˜í™˜
            paper.update({
                'title': title,
                'abstract': abstract,
                'likes': likes,
                'paper_link': paper_link,
                'code_link': code_link,
                'tags': tags,
                'description': abstract[:500] if abstract else ''  # ìš”ì•½
            })
            
        except Exception:
            pass
        
        return paper
    
    def _is_same_date(self, date_str: str, target_date: datetime) -> bool:
        """ë‚ ì§œ ë¬¸ìì—´ì´ target_dateì™€ ê°™ì€ ë‚ ì¸ì§€ í™•ì¸"""
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±
            for fmt in ['%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d']:
                try:
                    parsed = datetime.strptime(date_str[:19], fmt)
                    return parsed.date() == target_date.date()
                except (ValueError, TypeError):
                    continue
        except (ValueError, TypeError):
            pass
        return False
    
    def save_daily_data(self, papers: List[Dict], target_date: Optional[datetime] = None) -> Optional[str]:
        """
        ì¼ê°„ ë…¼ë¬¸ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None (ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ None)
        """
        # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
        if not papers:
            print("âš ï¸ ë…¼ë¬¸ì´ 0ê°œì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
        
        if target_date is None:
            target_date = datetime.utcnow()
        
        date_str = target_date.strftime('%Y-%m-%d')
        filename = f"daily-{date_str}.json"
        filepath = self.data_dir / filename
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‚´ìš© ë¹„êµ
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_urls = {p.get('url', '') for p in existing_data.get('papers', [])}
                    new_urls = {p.get('url', '') for p in papers if p.get('url')}
                    
                    # ë‚´ìš©ì´ ê°™ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
                    if existing_urls == new_urls:
                        print(f"ê¸°ì¡´ ë°ì´í„°ì™€ ë™ì¼: {filename} (ì €ì¥ ìŠ¤í‚µ)")
                        return None
            except Exception:
                pass
        
        data = {
            'date': date_str,
            'crawled_at': datetime.utcnow().isoformat(),
            'total_papers': len(papers),
            'papers': papers
        }
        
        filepath.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
        print(f"ì¼ê°„ ë°ì´í„° ì €ì¥: {filename} ({len(papers)}ê°œ ë…¼ë¬¸)")
        
        return str(filepath)
    
    def generate_monthly_summary(self, year: int, month: int) -> Dict:
        """
        ì›”ê°„ ìš”ì•½ ìƒì„±
        
        Args:
            year: ì—°ë„
            month: ì›” (1-12)
            
        Returns:
            ì›”ê°„ ìš”ì•½ ë°ì´í„°
        """
        # í•´ë‹¹ ì›”ì˜ ëª¨ë“  ì¼ê°„ ë°ì´í„° ë¡œë“œ
        daily_data = []
        start_date = datetime(year, month, 1)
        end_date = datetime(year, month + 1, 1) if month < 12 else datetime(year + 1, 1, 1)
        
        current_date = start_date
        while current_date < end_date:
            date_str = current_date.strftime('%Y-%m-%d')
            daily_file = self.data_dir / f"daily-{date_str}.json"
            
            if daily_file.exists():
                try:
                    with open(daily_file, 'r', encoding='utf-8') as f:
                        daily = json.load(f)
                        daily_data.append(daily)
                except Exception as e:
                    print(f"ì¼ê°„ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({date_str}): {e}")
            
            current_date += timedelta(days=1)
        
        # ëª¨ë“  ë…¼ë¬¸ ìˆ˜ì§‘
        all_papers = []
        paper_ids = set()
        
        for daily in daily_data:
            for paper in daily.get('papers', []):
                paper_id = paper.get('url', '') or paper.get('title', '')
                if paper_id and paper_id not in paper_ids:
                    paper_ids.add(paper_id)
                    all_papers.append(paper)
        
        # ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬
        all_papers.sort(key=lambda x: x.get('likes', 0), reverse=True)
        
        # í†µê³„
        total_papers = len(all_papers)
        total_likes = sum(p.get('likes', 0) for p in all_papers)
        top_papers = all_papers[:10]  # Top 10
        
        # íƒœê·¸ë³„ í†µê³„
        tag_stats = defaultdict(int)
        for paper in all_papers:
            for tag in paper.get('tags', []):
                tag_stats[tag] += 1
        
        top_tags = sorted(tag_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        
        summary = {
            'year': year,
            'month': month,
            'generated_at': datetime.utcnow().isoformat(),
            'total_papers': total_papers,
            'total_likes': total_likes,
            'average_likes': round(total_likes / total_papers, 2) if total_papers > 0 else 0,
            'days_crawled': len(daily_data),
            'top_papers': top_papers,
            'top_tags': [{'tag': tag, 'count': count} for tag, count in top_tags],
            'daily_summaries': [
                {
                    'date': d.get('date'),
                    'total_papers': d.get('total_papers', 0)
                }
                for d in daily_data
            ]
        }
        
        # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ JSON ì €ì¥í•˜ì§€ ì•ŠìŒ
        if total_papers == 0:
            print(f"âš ï¸ ì›”ê°„ ìš”ì•½ì— ë…¼ë¬¸ì´ 0ê°œì…ë‹ˆë‹¤. JSONì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return summary
        
        # ì›”ê°„ ìš”ì•½ ì €ì¥ (ë…¼ë¬¸ì´ ìˆì„ ë•Œë§Œ)
        filename = f"monthly-{year}-{month:02d}.json"
        filepath = self.data_dir / filename
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‚´ìš© ë¹„êµ
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_urls = {p.get('url', '') for p in existing_data.get('top_papers', [])}
                    new_urls = {p.get('url', '') for p in top_papers if p.get('url')}
                    
                    # ë‚´ìš©ì´ ê°™ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
                    if existing_urls == new_urls:
                        print(f"ê¸°ì¡´ ì›”ê°„ ìš”ì•½ ë°ì´í„°ì™€ ë™ì¼: {filename} (ì €ì¥ ìŠ¤í‚µ)")
                        return summary
            except Exception:
                pass
        
        filepath.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding='utf-8')
        print(f"ì›”ê°„ ìš”ì•½ ìƒì„±: {filename}")
        return summary
    
    def create_daily_summary_post(self, papers: List[Dict], target_date: Optional[datetime] = None, force_update: bool = False) -> Optional[str]:
        """
        ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
            target_date: ëŒ€ìƒ ë‚ ì§œ
            force_update: ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ì—…ë°ì´íŠ¸í• ì§€ ì—¬ë¶€
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ í¬ìŠ¤íŠ¸ ìƒì„±í•˜ì§€ ì•ŠìŒ
        if not papers:
            print("âš ï¸ ë…¼ë¬¸ì´ 0ê°œì…ë‹ˆë‹¤. í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
        
        if target_date is None:
            target_date = datetime.utcnow()
        
        # í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ì„¤ì •
        kst_date = target_date + timedelta(hours=9)
        post_date = kst_date.replace(hour=9, minute=15, second=0, microsecond=0)
        
        # íŒŒì¼ëª… ìƒì„±
        filename_date = target_date.strftime('%Y-%m-%d')
        filename = f"{filename_date}-daily-papers-summary.md"
        filepath = self.posts_dir / filename
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‚´ìš© ë¹„êµ
        if filepath.exists():
            # ê¸°ì¡´ íŒŒì¼ì˜ ë…¼ë¬¸ URL ëª©ë¡ ì¶”ì¶œ
            try:
                existing_content = filepath.read_text(encoding='utf-8')
                # ê¸°ì¡´ íŒŒì¼ì—ì„œ ë…¼ë¬¸ URL ì¶”ì¶œ
                existing_urls = set(re.findall(r'https://huggingface\.co/papers/[^\s\)]+', existing_content))
                # ìƒˆë¡œ í¬ë¡¤ë§í•œ ë…¼ë¬¸ URL ëª©ë¡
                new_urls = {paper.get('url', '') for paper in papers if paper.get('url')}
                
                # Abstractë„ ë¹„êµí•˜ì—¬ ë³€ê²½ì‚¬í•­ í™•ì¸
                existing_has_bad_abstract = 'Abstract: Join the discussion on this paper page' in existing_content
                new_has_good_abstract = any(
                    paper.get('abstract', '') and 
                    len(paper.get('abstract', '')) > 50 and
                    'Join the discussion' not in paper.get('abstract', '') and
                    'on this paper page' not in paper.get('abstract', '')
                    for paper in papers
                )
                
                # URLì´ ê°™ê³  Abstractê°€ ê°œì„ ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
                if existing_urls == new_urls and not new_has_good_abstract and not force_update:
                    print(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì¼ê°„ ìš”ì•½: {filename} (ë‚´ìš© ë™ì¼, ì—…ë°ì´íŠ¸ ìŠ¤í‚µ)")
                    return None
                
                # URLì´ ê°™ê³  Abstractê°€ ê°œì„ ë˜ì§€ ì•Šì•˜ê³  force_updateì—¬ë„ ìŠ¤í‚µ
                if existing_urls == new_urls and not new_has_good_abstract and force_update:
                    print(f"ê¸°ì¡´ íŒŒì¼ ë‚´ìš©ê³¼ ë™ì¼: {filename} (ì—…ë°ì´íŠ¸ ìŠ¤í‚µ)")
                    return None
                
                # Abstract ê°œì„ ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                if existing_urls == new_urls and existing_has_bad_abstract and new_has_good_abstract:
                    print(f"ê¸°ì¡´ íŒŒì¼ Abstract ê°œì„  í•„ìš”: {filename} (ì—…ë°ì´íŠ¸)")
                elif existing_urls != new_urls:
                    print(f"ê¸°ì¡´ íŒŒì¼ ë‚´ìš©ê³¼ ë‹¤ë¦„: {filename} (ì—…ë°ì´íŠ¸)")
            except Exception as e:
                print(f"ê¸°ì¡´ íŒŒì¼ í™•ì¸ ì˜¤ë¥˜: {e}, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ê¸°ì¡´ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if not filepath.exists():
            print(f"ìƒˆ íŒŒì¼ ìƒì„±: {filename}")
        
        # Front Matter
        front_matter = {
            'title': f'Hugging Face Daily Papers - {filename_date}',
            'date': f"{post_date.strftime('%Y-%m-%d %H:%M:%S')} +0900",
            'categories': ['Daily Papers', 'ì¼ê°„'],
            'tags': ['huggingface', 'papers', 'daily', 'ai'],
            'author': 'lim4349'
        }
        
        # ë³¸ë¬¸ ìƒì„±
        content = f"# Hugging Face Daily Papers - {filename_date}\n\n"
        content += f"ì´ **{len(papers)}ê°œ**ì˜ ë…¼ë¬¸ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n## ğŸ“Š ì¢‹ì•„ìš” ìˆœìœ„\n\n"
        
        for i, paper in enumerate(papers, 1):
            content += f"{i}. **{paper.get('title', 'Untitled')}** - ğŸ‘ {paper.get('likes', 0)}\n"
            content += f"   - [HF í˜ì´ì§€]({paper.get('url', '#')})\n"
            if paper.get('paper_link'):
                content += f"   - [ë…¼ë¬¸ ë§í¬]({paper['paper_link']})\n"
            if paper.get('abstract'):
                abstract = paper['abstract'][:200] + "..." if len(paper['abstract']) > 200 else paper['abstract']
                content += f"   - Abstract: {abstract}\n"
            content += "\n"
        
        # Front Matter + Content
        yaml_header = "---\n"
        for key, value in front_matter.items():
            yaml_header += f"{key}: {value}\n" if not isinstance(value, list) else f"{key}: {value}\n"
        yaml_header += "---\n\n"
        full_content = yaml_header + content
        
        filepath.write_text(full_content, encoding='utf-8')
        print(f"ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì €ì¥: {filename}")
        
        return str(filepath)
    
    def create_monthly_summary_post(self, summary: Dict, force_update: bool = False) -> Optional[str]:
        """
        ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            summary: ì›”ê°„ ìš”ì•½ ë°ì´í„°
            force_update: ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ì—…ë°ì´íŠ¸í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ í¬ìŠ¤íŠ¸ ìƒì„±í•˜ì§€ ì•ŠìŒ
        if summary['total_papers'] == 0:
            print("âš ï¸ ì›”ê°„ ìš”ì•½ì— ë…¼ë¬¸ì´ 0ê°œì…ë‹ˆë‹¤. í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
        
        year = summary['year']
        month = summary['month']
        date_str = f"{year}-{month:02d}"
        
        filename = f"{date_str}-01-monthly-papers-summary.md"
        filepath = self.posts_dir / filename
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‚´ìš© ë¹„êµ
        if filepath.exists():
            try:
                existing_content = filepath.read_text(encoding='utf-8')
                # ê¸°ì¡´ íŒŒì¼ì—ì„œ ë…¼ë¬¸ URL ì¶”ì¶œ
                existing_urls = set(re.findall(r'https://huggingface\.co/papers/[^\s\)]+', existing_content))
                # ìƒˆë¡œ ìƒì„±í•  ë…¼ë¬¸ URL ëª©ë¡
                new_urls = {paper.get('url', '') for paper in summary['top_papers'] if paper.get('url')}
                
                # ë‚´ìš©ì´ ê°™ìœ¼ë©´ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
                if existing_urls == new_urls and not force_update:
                    print(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì›”ê°„ ìš”ì•½: {filename} (ë‚´ìš© ë™ì¼, ì—…ë°ì´íŠ¸ ìŠ¤í‚µ)")
                    return None
                
                if existing_urls == new_urls and force_update:
                    print(f"ê¸°ì¡´ íŒŒì¼ ë‚´ìš©ê³¼ ë™ì¼: {filename} (ì—…ë°ì´íŠ¸ ìŠ¤í‚µ)")
                    return None
                
                print(f"ê¸°ì¡´ íŒŒì¼ ë‚´ìš©ê³¼ ë‹¤ë¦„: {filename} (ì—…ë°ì´íŠ¸)")
            except Exception as e:
                print(f"ê¸°ì¡´ íŒŒì¼ í™•ì¸ ì˜¤ë¥˜: {e}, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        
        if not filepath.exists():
            print(f"ìƒˆ ì›”ê°„ ìš”ì•½ ìƒì„±: {filename}")
        
        # Front Matter
        front_matter = {
            'title': f'Hugging Face Papers Monthly Summary - {year}ë…„ {month}ì›”',
            'date': f"{year}-{month:02d}-01 09:00:00 +0900",
            'categories': ['Daily Papers', 'ì›”ê°„'],
            'tags': ['huggingface', 'papers', 'monthly', 'ai', 'summary'],
            'author': 'lim4349'
        }
        
        # ë³¸ë¬¸ ìƒì„±
        content = f"# Hugging Face Papers ì›”ê°„ ìš”ì•½ - {year}ë…„ {month}ì›”\n\n## ğŸ“Š í†µê³„\n\n"
        content += f"- **ì´ ë…¼ë¬¸ ìˆ˜**: {summary['total_papers']}ê°œ\n"
        content += f"- **ì´ ì¢‹ì•„ìš” ìˆ˜**: {summary['total_likes']:,}\n"
        content += f"- **í‰ê·  ì¢‹ì•„ìš” ìˆ˜**: {summary['average_likes']:.2f}\n"
        content += f"- **ìˆ˜ì§‘ ì¼ìˆ˜**: {summary['days_crawled']}ì¼\n\n"
        content += "## ğŸ”¥ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë…¼ë¬¸ Top 10\n\n"
        
        if summary['top_papers']:
            for i, paper in enumerate(summary['top_papers'], 1):
                content += f"{i}. **{paper.get('title', 'Untitled')}** - ğŸ‘ {paper.get('likes', 0)}\n"
                content += f"   - [HF í˜ì´ì§€]({paper.get('url', '#')})\n\n"
        else:
            content += "ì´ë²ˆ ë‹¬ì— ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\n\n"
        
        if summary['top_tags']:
            content += "## ğŸ·ï¸ ì¸ê¸° íƒœê·¸ Top 10\n\n"
            for i, tag_info in enumerate(summary['top_tags'], 1):
                content += f"{i}. `{tag_info['tag']}` - {tag_info['count']}íšŒ\n"
            content += "\n"
        
        # Front Matter + Content
        yaml_header = "---\n"
        for key, value in front_matter.items():
            yaml_header += f"{key}: {value}\n" if not isinstance(value, list) else f"{key}: {value}\n"
        yaml_header += "---\n\n"
        full_content = yaml_header + content
        
        filepath.write_text(full_content, encoding='utf-8')
        print(f"ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì €ì¥: {filename}")
        
        return str(filepath)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 50)
    print("Hugging Face Daily Papers í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 50)
    
    crawler = HFDailyPapersCrawler()
    
    # ì˜¤ëŠ˜ ë‚ ì§œì˜ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°
    target_date = datetime.utcnow()
    print(f"\ní¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ: {target_date.strftime('%Y-%m-%d')}")
    
    papers = crawler.fetch_daily_papers(target_date)
    
    print(f"\nì´ {len(papers)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
    
    # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if not papers:
        print("âš ï¸ ë…¼ë¬¸ì´ 0ê°œì…ë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        print("\n" + "=" * 50)
        print("í¬ë¡¤ë§ ì™„ë£Œ: 0ê°œì˜ ë…¼ë¬¸ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        print("=" * 50)
        return
    
    # ë…¼ë¬¸ì´ ìˆìœ¼ë©´ ì¶œë ¥
    print("ì¢‹ì•„ìš” ìˆœìœ„ (Top 10):")
    for i, paper in enumerate(papers[:10], 1):
        likes = paper.get('likes', 0)
        title = paper.get('title', 'Unknown')[:60]
        print(f"  {i}. ğŸ‘ {likes} - {title}")
    
    # ë…¼ë¬¸ì´ ìˆì„ ë•Œë§Œ ë°ì´í„° ì €ì¥ ë° í¬ìŠ¤íŠ¸ ìƒì„±
    crawler.save_daily_data(papers, target_date)
    post_path = crawler.create_daily_summary_post(papers, target_date, force_update=False)
    if post_path:
        print(f"âœ… ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±/ì—…ë°ì´íŠ¸: {post_path}")
    else:
        print("â„¹ï¸ ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì—†ìŒ (ë‚´ìš© ë™ì¼ ë˜ëŠ” ì´ë¯¸ ì¡´ì¬)")
    
    # ì›”ê°„ ìš”ì•½ ìƒì„± (ì´ë²ˆ ë‹¬)
    try:
        current_year = target_date.year
        current_month = target_date.month
        summary = crawler.generate_monthly_summary(current_year, current_month)
        # ì›”ê°„ ìš”ì•½ì€ ë…¼ë¬¸ì´ ìˆì„ ë•Œë§Œ ìƒì„±
        if summary['total_papers'] > 0:
            monthly_post_path = crawler.create_monthly_summary_post(summary, force_update=False)
            if monthly_post_path:
                print(f"âœ… ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±/ì—…ë°ì´íŠ¸: {monthly_post_path}")
            else:
                print("â„¹ï¸ ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì—†ìŒ (ë‚´ìš© ë™ì¼ ë˜ëŠ” ì´ë¯¸ ì¡´ì¬)")
    except Exception as e:
        print(f"âš ï¸ ì›”ê°„ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
    
    print("\n" + "=" * 50)
    print(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(papers)}ê°œì˜ ë…¼ë¬¸ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    print("=" * 50)


if __name__ == "__main__":
    main()
