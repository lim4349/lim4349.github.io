#!/usr/bin/env python3
"""
Hugging Face Daily Papers í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
GitHub Actionsì—ì„œ ì‹¤í–‰í•˜ì—¬ ì¼ì¼ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ê³  Jekyll í¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
ì¼ê°„ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥í•˜ê³ , ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬í•˜ë©°, Abstractë„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import re
import json
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from collections import defaultdict
import time
try:
    import feedparser
except ImportError:
    feedparser = None


class HFDailyPapersCrawler:
    """Hugging Face Daily Papers í¬ë¡¤ëŸ¬"""
    
    def __init__(self, posts_dir: str = "_posts", data_dir: str = "_data/papers"):
        self.posts_dir = Path(posts_dir)
        self.posts_dir.mkdir(exist_ok=True)
        
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Hugging Face Daily Papers ê´€ë ¨ URLë“¤
        self.base_url = "https://huggingface.co"
        self.papers_url = "https://huggingface.co/papers"
        # ì—¬ëŸ¬ RSS í”¼ë“œ URL ì‹œë„
        self.rss_urls = [
            "https://huggingface.co/blog/tags/papers/rss.xml",
            "https://huggingface.co/blog/rss.xml",  # ì „ì²´ ë¸”ë¡œê·¸ RSS
        ]
        self.rss_url = self.rss_urls[0]  # ê¸°ë³¸ê°’
    
    def fetch_daily_papers(self, target_date: Optional[datetime] = None) -> List[Dict]:
        """
        íŠ¹ì • ë‚ ì§œì˜ ë…¼ë¬¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì¢‹ì•„ìš” ìˆ˜ í¬í•¨).
        
        Args:
            target_date: í¬ë¡¤ë§í•  ë‚ ì§œ (Noneì´ë©´ ì˜¤ëŠ˜)
            
        Returns:
            ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ (ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬ë¨)
        """
        if target_date is None:
            target_date = datetime.utcnow()
        
        papers = []
        
        # RSS í”¼ë“œì—ì„œ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°
        if feedparser:
            try:
                papers.extend(self._fetch_from_rss(target_date))
            except Exception:
                pass
        
        # ì›¹ í˜ì´ì§€ì—ì„œ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°
        try:
            papers.extend(self._fetch_daily_from_web(target_date))
        except Exception:
            pass
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_papers = []
        for paper in papers:
            url = paper.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_papers.append(paper)
        
        # ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
        unique_papers.sort(key=lambda x: x.get('likes', 0), reverse=True)
        
        # ìƒìœ„ 10ê°œë§Œ ì„ íƒ
        top_papers = unique_papers[:10]
        print(f"\n[ì„ íƒ] ì´ {len(unique_papers)}ê°œ ì¤‘ ìƒìœ„ 10ê°œ ì„ íƒ")
        
        # ìƒìœ„ 10ê°œë§Œ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (Abstract í¬í•¨)
        enriched_papers = []
        for i, paper in enumerate(top_papers, 1):
            try:
                print(f"  [{i}/10] ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {paper.get('title', 'Unknown')[:50]}...")
                enriched = self._enrich_paper_details(paper)
                enriched_papers.append(enriched)
                # ìš”ì²­ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(1)
            except Exception as e:
                print(f"  ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({paper.get('url', 'unknown')}): {e}")
                enriched_papers.append(paper)
        
        # ìµœì¢… ì •ë ¬ (ì¢‹ì•„ìš” ìˆ˜ë¡œ)
        enriched_papers.sort(key=lambda x: x.get('likes', 0), reverse=True)
        
        print(f"\n[ì™„ë£Œ] ìµœì¢… {len(enriched_papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return enriched_papers
    
    def _fetch_from_rss(self, target_date: datetime) -> List[Dict]:
        """RSS í”¼ë“œì—ì„œ íŠ¹ì • ë‚ ì§œì˜ ë…¼ë¬¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if feedparser is None:
            return []
        
        papers = []
        feed = None
        
        # RSS URL ì‹œë„
        for rss_url in self.rss_urls:
            try:
                feed = feedparser.parse(rss_url)
                if feed.entries:
                    print(f"RSS í”¼ë“œ: {len(feed.entries)}ê°œ í•­ëª©")
                    break
            except Exception:
                continue
        
        if not feed or not feed.entries:
            return papers
        
        target_date_str = target_date.strftime('%Y-%m-%d')
        yesterday = (target_date - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # ë‚ ì§œ ë§¤ì¹­
        for entry in feed.entries[:30]:
            entry_date = entry.get('published', '')
            if (target_date_str in entry_date or yesterday in entry_date or
                self._is_same_date(entry_date, target_date) or
                self._is_same_date(entry_date, target_date - timedelta(days=1))):
                papers.append({
                    'title': entry.get('title', ''),
                    'url': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'summary': entry.get('summary', ''),
                    'likes': 0
                })
        
        # ë‚ ì§œ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ìµœì‹  10ê°œ ì‚¬ìš©
        if not papers:
            for entry in feed.entries[:10]:
                papers.append({
                    'title': entry.get('title', ''),
                    'url': entry.get('link', ''),
                    'published': entry.get('published', datetime.utcnow().isoformat()),
                    'summary': entry.get('summary', ''),
                    'likes': 0
                })
        
        return papers
    
    def check_date_has_papers(self, target_date: datetime) -> bool:
        """
        íŠ¹ì • ë‚ ì§œì— ë…¼ë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸
        
        Args:
            target_date: í™•ì¸í•  ë‚ ì§œ
            
        Returns:
            ë…¼ë¬¸ì´ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False
        """
        date_str = target_date.strftime('%Y-%m-%d')
        url = f"{self.papers_url}/date/{date_str}"
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(url, timeout=30, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # í˜ì´ì§€ì˜ ë‚ ì§œ ì œëª© í™•ì¸ (h1 ë˜ëŠ” íŠ¹ì • í´ë˜ìŠ¤)
            # í˜ì´ì§€ì— ë‚ ì§œê°€ í‘œì‹œë˜ê³  ë…¼ë¬¸ í•­ëª©ì´ ìˆìœ¼ë©´ True
            paper_headings = soup.find_all('h3')
            
            # ë…¼ë¬¸ ì œëª©ì´ ìˆëŠ”ì§€ í™•ì¸ (ìµœì†Œ 1ê°œ ì´ìƒ)
            if paper_headings:
                # ì‹¤ì œë¡œ ë…¼ë¬¸ ì œëª©ì¸ì§€ í™•ì¸ (ë„ˆë¬´ ì§§ê±°ë‚˜ íŠ¹ì • íŒ¨í„´ì´ë©´ ì œì™¸)
                valid_headings = [
                    h for h in paper_headings 
                    if h.find('a') and len(h.get_text(strip=True)) > 10
                ]
                if len(valid_headings) >= 1:
                    return True
            
            return False
        except Exception as e:
            print(f"ë‚ ì§œ í™•ì¸ ì‹¤íŒ¨ ({url}): {e}")
            return False
    
    def _fetch_daily_from_web(self, target_date: datetime) -> List[Dict]:
        """ì›¹ í˜ì´ì§€ì—ì„œ ì¼ì¼ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        papers = []
        
        # ì˜¬ë°”ë¥¸ URL í˜•ì‹: https://huggingface.co/papers/date/YYYY-MM-DD
        # ì‹¤ì œ ì˜ˆì‹œ: https://huggingface.co/papers/date/2025-10-31
        date_str = target_date.strftime('%Y-%m-%d')
        url = f"{self.papers_url}/date/{date_str}"
        
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
            response = requests.get(url, timeout=30, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë…¼ë¬¸ í•­ëª© ì°¾ê¸° (h3 íƒœê·¸ë¡œ ì œëª© ì°¾ê¸°)
            # êµ¬ì¡°: h3 > a (ì œëª© ë§í¬), ì¢‹ì•„ìš” ìˆ˜ëŠ” ê°™ì€ ë ˆë²¨ì— ìˆìŒ
            paper_headings = soup.find_all('h3')
            
            
            for heading in paper_headings:
                try:
                    # ì œëª© ë§í¬ ì°¾ê¸°
                    title_link = heading.find('a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text(strip=True)
                    if not title:
                        continue
                    
                    paper_url = title_link.get('href', '')
                    
                    if not paper_url.startswith('http'):
                        if paper_url.startswith('/'):
                            paper_url = self.base_url + paper_url
                        else:
                            paper_url = f"{self.base_url}/papers/{paper_url}"
                    
                    # ì¢‹ì•„ìš” ìˆ˜ ì°¾ê¸° - h3ì˜ ë¶€ëª¨ ìš”ì†Œì—ì„œ ì°¾ê¸°
                    likes = 0
                    parent = heading.parent
                    if parent:
                        # ë¶€ëª¨ ìš”ì†Œì˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì°¾ê¸°
                        parent_text = parent.get_text()
                        # ì¢‹ì•„ìš” ìˆ˜ëŠ” ë³´í†µ í° ìˆ«ìì´ê³ , h3 ê·¼ì²˜ì— ìˆìŒ
                        # í˜•ì‹: "97\n" ë˜ëŠ” " 97 " ê°™ì€ íŒ¨í„´
                        numbers = re.findall(r'\b(\d+)\b', parent_text)
                        
                        # h3 ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ìˆ«ìê°€ ì¢‹ì•„ìš” ìˆ˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                        # ë˜ëŠ” h3 ì•ì— ìˆëŠ” í° ìˆ«ì
                        if numbers:
                            # ì²« ë²ˆì§¸ í° ìˆ«ì (10 ì´ìƒ)ë¥¼ ì¢‹ì•„ìš” ìˆ˜ë¡œ ì¶”ì •
                            for num_str in numbers:
                                try:
                                    num = int(num_str)
                                    if num >= 10:  # ì¢‹ì•„ìš”ëŠ” ë³´í†µ 10 ì´ìƒ
                                        likes = num
                                        break
                                except (ValueError, TypeError):
                                    pass
                        
                        # h3ì˜ ë‹¤ìŒ í˜•ì œ ìš”ì†Œ í™•ì¸
                        if likes == 0:
                            current = heading.next_sibling
                            checked = 0
                            while current and checked < 3:
                                if hasattr(current, 'get_text'):
                                    text = current.get_text(strip=True)
                                    like_match = re.search(r'^(\d+)$', text)
                                    if like_match:
                                        num = int(like_match.group(1))
                                        if num >= 10:
                                            likes = num
                                            break
                                current = getattr(current, 'next_sibling', None)
                                checked += 1
                    
                    # ê¸°ê´€ ì •ë³´ ì¶”ì¶œ - h3 ë‹¤ìŒì— ì˜¤ëŠ” í…ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
                    institution = ''
                    if parent:
                        # h3 ë‹¤ìŒì— ì˜¤ëŠ” ëª¨ë“  í˜•ì œ ìš”ì†Œ í™•ì¸
                        current = heading.next_sibling
                        checked = 0
                        while current and checked < 10:
                            if hasattr(current, 'get_text'):
                                text = current.get_text(strip=True)
                                # ê¸°ê´€ëª…ì€ ë³´í†µ í…ìŠ¤íŠ¸ì´ê³ , ë§í¬ë‚˜ íŠ¹ì • êµ¬ì¡°ë¥¼ ê°€ì§
                                # "ByteDance-Seed ByteDance Seed" ê°™ì€ íŒ¨í„´
                                if text and len(text) > 2 and len(text) < 200:
                                    # ìˆ«ìë‚˜ íŠ¹ìˆ˜ ë¬¸ìë§Œ ìˆëŠ” ê²ƒì€ ì œì™¸
                                    if not re.match(r'^[\d\s\-]+$', text):
                                        # ë§í¬ë‚˜ í…ìŠ¤íŠ¸ ë…¸ë“œì—ì„œ ì¶”ì¶œ
                                        if hasattr(current, 'find'):
                                            # ë§í¬ ì•ˆì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                            link = current.find('a')
                                            if link:
                                                link_text = link.get_text(strip=True)
                                                if link_text and len(link_text) > 2:
                                                    institution = link_text
                                                    break
                                            else:
                                                # ì¼ë°˜ í…ìŠ¤íŠ¸ ë…¸ë“œ
                                                institution = text
                                                break
                                        else:
                                            institution = text
                                            break
                            current = getattr(current, 'next_sibling', None)
                            checked += 1
                        
                        # ë¶€ëª¨ ìš”ì†Œì—ì„œ ê¸°ê´€ ê´€ë ¨ ë§í¬ë‚˜ í…ìŠ¤íŠ¸ ì°¾ê¸°
                        if not institution:
                            # a íƒœê·¸ ì¤‘ hrefì— íŠ¹ì • íŒ¨í„´ì´ ìˆëŠ” ê²ƒ ì°¾ê¸° (org, company ë“±)
                            for link in parent.find_all('a', href=True):
                                href = link.get('href', '')
                                link_text = link.get_text(strip=True)
                                # ê¸°ê´€ í˜ì´ì§€ ë§í¬ íŒ¨í„´ì´ë‚˜ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
                                if (link_text and len(link_text) > 2 and len(link_text) < 200 and
                                    ('/org/' in href or '/company/' in href or 
                                     not href.startswith('/papers/') and not href.startswith('#'))):
                                    institution = link_text
                                    break
                            
                            # ë¶€ëª¨ ìš”ì†Œì˜ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ê´€ëª… íŒ¨í„´ ì°¾ê¸°
                            if not institution:
                                parent_text = parent.get_text(separator=' ', strip=True)
                                # ì œëª©ê³¼ ì¢‹ì•„ìš” ìˆ˜ ì‚¬ì´ì˜ í…ìŠ¤íŠ¸ì—ì„œ ê¸°ê´€ëª… ì°¾ê¸°
                                lines = parent_text.split('\n')
                                for line in lines:
                                    line = line.strip()
                                    # ì œëª©ì´ ì•„ë‹Œ ê¸´ í…ìŠ¤íŠ¸ ë¼ì¸ ì°¾ê¸°
                                    if (line and len(line) > 3 and len(line) < 200 and 
                                        title.lower() not in line.lower() and
                                        not re.match(r'^[\d\s]+$', line)):
                                        # ë§í¬ë‚˜ íŠ¹ìˆ˜ ë¬¸ìê°€ ì•„ë‹Œ ì¼ë°˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš°
                                        if not line.startswith('http') and not line.startswith('/'):
                                            institution = line
                                            break
                    
                    if title and paper_url:
                        papers.append({
                            'title': title,
                            'url': paper_url,
                            'published': target_date.isoformat(),
                            'likes': likes,
                            'institution': institution
                        })
                except Exception:
                    continue
                    
        except Exception:
            pass
        
        return papers
    
    def _enrich_paper_details(self, paper: Dict) -> Dict:
        """ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (Abstract í¬í•¨)"""
        url = paper.get('url', '')
        if not url:
            return paper
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, timeout=30, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì¢‹ì•„ìš” ìˆ˜, ë…¼ë¬¸ ë§í¬, ì½”ë“œ ë§í¬, íƒœê·¸ ì¶”ì¶œ
            likes = paper.get('likes', 0)
            for elem in soup.find_all(['span', 'div', 'button'], class_=re.compile(r'like|favorite', re.I)):
                match = re.search(r'(\d+)', elem.get_text(strip=True))
                if match:
                    likes = max(likes, int(match.group(1)))
            
            title = paper.get('title', '')
            if not title:
                h1 = soup.find('h1') or soup.find('title')
                if h1:
                    title = h1.get_text(strip=True)
            
            paper_link = paper.get('paper_link', '')
            # arXiv, PDF, DOI ë§í¬ ì°¾ê¸°
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                # arXiv ë§í¬ (abs/, pdf/, e-print ë“±)
                if re.search(r'arxiv\.org|arxiv\.org/abs/|arxiv\.org/pdf/', href, re.I):
                    paper_link = href
                    if not paper_link.startswith('http'):
                        # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
                        if href.startswith('/abs/') or href.startswith('/pdf/'):
                            paper_link = 'https://arxiv.org' + href
                        elif href.startswith('/'):
                            paper_link = 'https://arxiv.org/abs/' + href.lstrip('/')
                    break
                # PDF ë§í¬
                elif re.search(r'\.pdf$', href, re.I) and not paper_link:
                    paper_link = href
                    if not paper_link.startswith('http'):
                        paper_link = self.base_url + paper_link if paper_link.startswith('/') else href
                # DOI ë§í¬
                elif re.search(r'doi\.org|doi:', href, re.I) and not paper_link:
                    paper_link = href
                    if not paper_link.startswith('http'):
                        paper_link = 'https://doi.org' + href if href.startswith('/') else href
            
            code_link = paper.get('code_link', '')
            for link in soup.find_all('a', href=re.compile(r'(github|gitlab)', re.I)):
                code_link = link.get('href', '')
                break
            
            tags = paper.get('tags', [])
            if not tags:
                tags = [tag.get_text(strip=True) for tag in soup.find_all(['a', 'span'], class_=re.compile(r'tag', re.I))[:10]]
            
            # ê¸°ê´€ ì •ë³´ëŠ” ëª©ë¡ í˜ì´ì§€ì—ì„œë§Œ ì¶”ì¶œ (ìƒì„¸ í˜ì´ì§€ì—ì„œëŠ” ì°¾ì§€ ì•ŠìŒ)
            institution = paper.get('institution', '')
            
            # Abstract ì¶”ì¶œ: ë…¼ë¬¸ ë§í¬(paper_link)ì—ì„œ ë¨¼ì € ì‹œë„
            abstract = paper.get('abstract', '')
            
            # 1. ë…¼ë¬¸ ë§í¬(arXiv ë“±)ì—ì„œ Abstract ì¶”ì¶œ ì‹œë„
            if paper_link:
                try:
                    paper_response = requests.get(paper_link, timeout=30, headers=headers)
                    paper_response.raise_for_status()
                    paper_soup = BeautifulSoup(paper_response.content, 'html.parser')
                    
                    # arXiv í˜ì´ì§€ì˜ Abstract ì¶”ì¶œ
                    # arXivëŠ” ë³´í†µ <blockquote class="abstract"> ë˜ëŠ” <div class="abstract"> ì‚¬ìš©
                    abstract_elem = None
                    
                    # arXiv í˜•ì‹ 1: blockquote.abstract
                    abstract_elem = paper_soup.find('blockquote', class_='abstract')
                    if not abstract_elem:
                        # arXiv í˜•ì‹ 2: div.abstract
                        abstract_elem = paper_soup.find('div', class_='abstract')
                    if not abstract_elem:
                        # arXiv í˜•ì‹ 3: span.abstract-text
                        abstract_elem = paper_soup.find('span', class_='abstract-text')
                    if not abstract_elem:
                        # arXiv í˜•ì‹ 4: classì— abstract í¬í•¨
                        abstract_elem = paper_soup.find(class_=re.compile(r'abstract', re.I))
                    
                    if abstract_elem:
                        abstract_text = abstract_elem.get_text(separator=' ', strip=True)
                        # "Abstract:" ê°™ì€ ë ˆì´ë¸” ì œê±°
                        abstract_text = re.sub(r'^Abstract\s*:?\s*', '', abstract_text, flags=re.I)
                        # "arXiv:" ê°™ì€ ì ‘ë‘ì‚¬ ì œê±°
                        abstract_text = re.sub(r'^arXiv\s*:?\s*\d+\.\d+\s*', '', abstract_text, flags=re.I)
                        if len(abstract_text) >= 50:
                            abstract = abstract_text.strip()
                            print(f"    âœ… ë…¼ë¬¸ í˜ì´ì§€ì—ì„œ Abstract ì¶”ì¶œ ì„±ê³µ ({len(abstract)}ì)")
                except Exception as e:
                    print(f"    âš ï¸ ë…¼ë¬¸ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨ ({paper_link}): {e}")
            
            # 2. ë…¼ë¬¸ ë§í¬ì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ Hugging Face í˜ì´ì§€ì—ì„œ ì‹œë„
            if not abstract or len(abstract) < 50:
                abstract_candidates = []
                
                # ëª…ì‹œì ì¸ abstract/summary ì„¹ì…˜ ì°¾ê¸°
                for selector in [
                    'div[class*="abstract"]',
                    'div[class*="summary"]',
                    'section[class*="abstract"]',
                    'section[class*="summary"]',
                    'p[class*="abstract"]',
                    'p[class*="summary"]',
                ]:
                    for elem in soup.select(selector):
                        text = elem.get_text(separator=' ', strip=True)
                        if text and 100 <= len(text) <= 2000:
                            abstract_candidates.append(text)
                
                # main/articleì—ì„œ ì²« ë²ˆì§¸ ê¸´ ë¬¸ë‹¨ ì°¾ê¸°
                for container in soup.find_all(['main', 'article']):
                    paragraphs = container.find_all(['p', 'div'], limit=10)
                    for p in paragraphs:
                        text = p.get_text(separator=' ', strip=True)
                        if (text and 150 <= len(text) <= 2000 and
                            'Join the discussion' not in text and
                            'Subscribe' not in text and
                            'Get trending papers' not in text and
                            'on this paper page' not in text and
                            not text.startswith('http') and
                            len(text.split()) >= 20):
                            abstract_candidates.append(text)
                            break
                
                # ê°€ì¥ ì í•©í•œ í›„ë³´ ì„ íƒ
                if abstract_candidates:
                    filtered = []
                    unwanted_phrases = [
                        'Join the discussion',
                        'on this paper page',
                        'Subscribe',
                        'Get trending papers',
                        'View on',
                        'Download',
                        'Like',
                        'Share',
                    ]
                    
                    for candidate in abstract_candidates:
                        has_unwanted = any(phrase.lower() in candidate.lower() for phrase in unwanted_phrases)
                        if not has_unwanted and len(candidate) >= 100:
                            filtered.append(candidate)
                    
                    if filtered:
                        abstract = max(filtered, key=len)
                    elif abstract_candidates:
                        abstract = max(abstract_candidates, key=len)
                        for unwanted in unwanted_phrases:
                            if unwanted.lower() in abstract.lower():
                                parts = re.split(re.escape(unwanted), abstract, flags=re.I)
                                if parts:
                                    abstract = parts[0].strip()
                                    break
                
                # ë©”íƒ€ íƒœê·¸ ì‹œë„ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
                if not abstract or len(abstract) < 50:
                    for meta in soup.find_all('meta', attrs={'name': ['description'], 'property': ['og:description']}):
                        desc = meta.get('content', '').strip()
                        if (desc and len(desc) >= 100 and
                            'Join the discussion' not in desc and
                            'on this paper page' not in desc):
                            abstract = desc
                            break
            
            # 3. ìµœì¢… ì •ë¦¬
            if abstract:
                # ë¶ˆí•„ìš”í•œ êµ¬ë¬¸ ì œê±°
                unwanted_patterns = [
                    r'Join the discussion.*?$',
                    r'on this paper page.*?$',
                    r'Subscribe.*?$',
                    r'Get trending papers.*?$',
                ]
                for pattern in unwanted_patterns:
                    abstract = re.sub(pattern, '', abstract, flags=re.I).strip()
                
                # ë„ˆë¬´ ì§§ê±°ë‚˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                if (len(abstract) < 50 or
                    abstract.lower() in ['join the discussion', 'subscribe', 'get trending papers'] or
                    'on this paper page' in abstract.lower()):
                    abstract = ''
            
            
            # ì—…ë°ì´íŠ¸ëœ ì •ë³´ ë°˜í™˜
            paper.update({
                'title': title,
                'abstract': abstract,
                'likes': likes,
                'paper_link': paper_link,
                'code_link': code_link,
                'tags': tags,
                'institution': institution,
                'description': abstract[:500] if abstract else ''  # ìš”ì•½
            })
            
        except Exception:
            pass
        
        return paper
    
    def _is_same_date(self, date_str: str, target_date: datetime) -> bool:
        """ë‚ ì§œ ë¬¸ìì—´ì´ target_dateì™€ ê°™ì€ ë‚ ì¸ì§€ í™•ì¸"""
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±
            for fmt in ['%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d']:
                try:
                    parsed = datetime.strptime(date_str[:19], fmt)
                    return parsed.date() == target_date.date()
                except (ValueError, TypeError):
                    continue
        except (ValueError, TypeError):
            pass
        return False
    
    def is_weekend(self, target_date: datetime) -> bool:
        """
        íŠ¹ì • ë‚ ì§œê°€ ì£¼ë§(í† ìš”ì¼, ì¼ìš”ì¼)ì¸ì§€ í™•ì¸
        
        Args:
            target_date: í™•ì¸í•  ë‚ ì§œ
            
        Returns:
            ì£¼ë§ì´ë©´ True, í‰ì¼ì´ë©´ False
        """
        # weekday(): ì›”ìš”ì¼=0, ì¼ìš”ì¼=6
        # í† ìš”ì¼=5, ì¼ìš”ì¼=6ì´ë©´ ì£¼ë§
        return target_date.weekday() >= 5
    
    def has_existing_post(self, target_date: datetime) -> bool:
        """
        íŠ¹ì • ë‚ ì§œì˜ í¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        
        Args:
            target_date: í™•ì¸í•  ë‚ ì§œ
            
        Returns:
            íŒŒì¼ì´ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False
        """
        date_str = target_date.strftime('%Y-%m-%d')
        filename = f"{date_str}-daily-papers-summary.md"
        filepath = self.posts_dir / filename
        return filepath.exists()
    
    def save_daily_data(self, papers: List[Dict], target_date: Optional[datetime] = None) -> Optional[str]:
        """
        ì¼ê°„ ë…¼ë¬¸ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None (ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ None)
        """
        # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
        if not papers:
            print("âš ï¸ ë…¼ë¬¸ì´ 0ê°œì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
        
        if target_date is None:
            target_date = datetime.utcnow()
        
        date_str = target_date.strftime('%Y-%m-%d')
        filename = f"daily-{date_str}.json"
        filepath = self.data_dir / filename
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‚´ìš© ë¹„êµ
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_urls = {p.get('url', '') for p in existing_data.get('papers', [])}
                    new_urls = {p.get('url', '') for p in papers if p.get('url')}
                    
                    # ë‚´ìš©ì´ ê°™ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
                    if existing_urls == new_urls:
                        print(f"ê¸°ì¡´ ë°ì´í„°ì™€ ë™ì¼: {filename} (ì €ì¥ ìŠ¤í‚µ)")
                        return None
            except Exception:
                pass
        
        data = {
            'date': date_str,
            'crawled_at': datetime.utcnow().isoformat(),
            'total_papers': len(papers),
            'papers': papers
        }
        
        filepath.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
        print(f"ì¼ê°„ ë°ì´í„° ì €ì¥: {filename} ({len(papers)}ê°œ ë…¼ë¬¸)")
        
        return str(filepath)
    
    def generate_monthly_summary(self, year: int, month: int) -> Dict:
        """
        ì›”ê°„ ìš”ì•½ ìƒì„±
        
        Args:
            year: ì—°ë„
            month: ì›” (1-12)
            
        Returns:
            ì›”ê°„ ìš”ì•½ ë°ì´í„°
        """
        # í•´ë‹¹ ì›”ì˜ ëª¨ë“  ì¼ê°„ ë°ì´í„° ë¡œë“œ
        daily_data = []
        start_date = datetime(year, month, 1)
        end_date = datetime(year, month + 1, 1) if month < 12 else datetime(year + 1, 1, 1)
        
        current_date = start_date
        while current_date < end_date:
            date_str = current_date.strftime('%Y-%m-%d')
            daily_file = self.data_dir / f"daily-{date_str}.json"
            
            if daily_file.exists():
                try:
                    with open(daily_file, 'r', encoding='utf-8') as f:
                        daily = json.load(f)
                        daily_data.append(daily)
                except Exception as e:
                    print(f"ì¼ê°„ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({date_str}): {e}")
            
            current_date += timedelta(days=1)
        
        # ëª¨ë“  ë…¼ë¬¸ ìˆ˜ì§‘
        all_papers = []
        paper_ids = set()
        
        for daily in daily_data:
            for paper in daily.get('papers', []):
                paper_id = paper.get('url', '') or paper.get('title', '')
                if paper_id and paper_id not in paper_ids:
                    paper_ids.add(paper_id)
                    all_papers.append(paper)
        
        # ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬
        all_papers.sort(key=lambda x: x.get('likes', 0), reverse=True)
        
        # í†µê³„
        total_papers = len(all_papers)
        total_likes = sum(p.get('likes', 0) for p in all_papers)
        top_papers = all_papers[:20]  # Top 20
        
        # íƒœê·¸ë³„ í†µê³„
        tag_stats = defaultdict(int)
        for paper in all_papers:
            for tag in paper.get('tags', []):
                tag_stats[tag] += 1
        
        top_tags = sorted(tag_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        
        summary = {
            'year': year,
            'month': month,
            'generated_at': datetime.utcnow().isoformat(),
            'total_papers': total_papers,
            'total_likes': total_likes,
            'average_likes': round(total_likes / total_papers, 2) if total_papers > 0 else 0,
            'days_crawled': len(daily_data),
            'top_papers': top_papers,
            'top_tags': [{'tag': tag, 'count': count} for tag, count in top_tags],
            'daily_summaries': [
                {
                    'date': d.get('date'),
                    'total_papers': d.get('total_papers', 0)
                }
                for d in daily_data
            ]
        }
        
        # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ JSON ì €ì¥í•˜ì§€ ì•ŠìŒ
        if total_papers == 0:
            print("âš ï¸ ì›”ê°„ ìš”ì•½ì— ë…¼ë¬¸ì´ 0ê°œì…ë‹ˆë‹¤. JSONì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return summary
        
        # ì›”ê°„ ìš”ì•½ ì €ì¥ (ë…¼ë¬¸ì´ ìˆì„ ë•Œë§Œ)
        filename = f"monthly-{year}-{month:02d}.json"
        filepath = self.data_dir / filename
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‚´ìš© ë¹„êµ
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
                    existing_urls = {p.get('url', '') for p in existing_data.get('top_papers', [])}
                    new_urls = {p.get('url', '') for p in top_papers if p.get('url')}
                    
                    # ë‚´ìš©ì´ ê°™ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
                    if existing_urls == new_urls:
                        print(f"ê¸°ì¡´ ì›”ê°„ ìš”ì•½ ë°ì´í„°ì™€ ë™ì¼: {filename} (ì €ì¥ ìŠ¤í‚µ)")
                        return summary
            except Exception:
                pass
        
        filepath.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding='utf-8')
        print(f"ì›”ê°„ ìš”ì•½ ìƒì„±: {filename}")
        return summary
    
    def create_daily_summary_post(self, papers: List[Dict], target_date: Optional[datetime] = None, force_update: bool = True) -> Optional[str]:
        """
        ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            papers: ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
            target_date: ëŒ€ìƒ ë‚ ì§œ
            force_update: ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ì—…ë°ì´íŠ¸í• ì§€ ì—¬ë¶€
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ í¬ìŠ¤íŠ¸ ìƒì„±í•˜ì§€ ì•ŠìŒ
        if not papers:
            print("âš ï¸ ë…¼ë¬¸ì´ 0ê°œì…ë‹ˆë‹¤. í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
        
        if target_date is None:
            target_date = datetime.utcnow()
        
        # í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ë‚ ì§œ ì„¤ì •
        kst_date = target_date + timedelta(hours=9)
        post_date = kst_date.replace(hour=9, minute=15, second=0, microsecond=0)
        
        # íŒŒì¼ëª… ìƒì„±
        filename_date = target_date.strftime('%Y-%m-%d')
        filename = f"{filename_date}-daily-papers-summary.md"
        filepath = self.posts_dir / filename
        
        # force_updateê°€ Trueë©´ ë¬´ì¡°ê±´ ë®ì–´ì“°ê¸° (ì¢‹ì•„ìš” ìˆ˜ ì—…ë°ì´íŠ¸ìš©)
        if force_update and filepath.exists():
            print(f"ê¸°ì¡´ íŒŒì¼ ê°•ì œ ì—…ë°ì´íŠ¸: {filename} (ì¢‹ì•„ìš” ìˆ˜ ë“± ì—…ë°ì´íŠ¸)")
        elif filepath.exists():
            # ê¸°ì¡´ íŒŒì¼ì´ ìˆê³  force_updateê°€ Falseì¼ ë•Œë§Œ ë‚´ìš© ë¹„êµí•˜ì—¬ ìŠ¤í‚µ ì—¬ë¶€ ê²°ì •
            try:
                existing_content = filepath.read_text(encoding='utf-8')
                # ê¸°ì¡´ íŒŒì¼ì—ì„œ ë…¼ë¬¸ URL ì¶”ì¶œ
                existing_urls = set(re.findall(r'https://huggingface\.co/papers/[^\s\)]+', existing_content))
                # ìƒˆë¡œ í¬ë¡¤ë§í•œ ë…¼ë¬¸ URL ëª©ë¡
                new_urls = {paper.get('url', '') for paper in papers if paper.get('url')}
                
                # Abstractë„ ë¹„êµí•˜ì—¬ ë³€ê²½ì‚¬í•­ í™•ì¸
                existing_has_bad_abstract = 'Abstract: Join the discussion on this paper page' in existing_content
                new_has_good_abstract = any(
                    paper.get('abstract', '') and 
                    len(paper.get('abstract', '')) > 50 and
                    'Join the discussion' not in paper.get('abstract', '') and
                    'on this paper page' not in paper.get('abstract', '')
                    for paper in papers
                )
                
                # URLì´ ê°™ê³  Abstractê°€ ê°œì„ ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
                if existing_urls == new_urls and not new_has_good_abstract:
                    print(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì¼ê°„ ìš”ì•½: {filename} (ë‚´ìš© ë™ì¼, ì—…ë°ì´íŠ¸ ìŠ¤í‚µ)")
                    return None
                
                # Abstract ê°œì„ ì´ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                if existing_urls == new_urls and existing_has_bad_abstract and new_has_good_abstract:
                    print(f"ê¸°ì¡´ íŒŒì¼ Abstract ê°œì„  í•„ìš”: {filename} (ì—…ë°ì´íŠ¸)")
                elif existing_urls != new_urls:
                    print(f"ê¸°ì¡´ íŒŒì¼ ë‚´ìš©ê³¼ ë‹¤ë¦„: {filename} (ì—…ë°ì´íŠ¸)")
            except Exception as e:
                print(f"ê¸°ì¡´ íŒŒì¼ í™•ì¸ ì˜¤ë¥˜: {e}, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        
        # ê¸°ì¡´ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if not filepath.exists():
            print(f"ìƒˆ íŒŒì¼ ìƒì„±: {filename}")
        
        # Front Matter
        front_matter = {
            'title': f'Hugging Face Daily Papers - {filename_date}',
            'date': f"{post_date.strftime('%Y-%m-%d %H:%M:%S')} +0900",
            'categories': ['Daily Papers', 'ì¼ê°„'],
            'tags': ['huggingface', 'papers', 'daily', 'ai'],
            'author': 'lim4349'
        }
        
        # ë³¸ë¬¸ ìƒì„±
        content = f"# Hugging Face Daily Papers - {filename_date}\n\n"
        content += f"ì´ **{len(papers)}ê°œ**ì˜ ë…¼ë¬¸ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n## ğŸ“Š ì¢‹ì•„ìš” ìˆœìœ„\n\n"
        
        for i, paper in enumerate(papers, 1):
            content += f"{i}. **{paper.get('title', 'Untitled')}** - ğŸ‘ {paper.get('likes', 0)}\n"
            if paper.get('institution'):
                content += f"   - ê¸°ê´€: {paper['institution']}\n"
            content += f"   - [HF í˜ì´ì§€]({paper.get('url', '#')})\n"
            if paper.get('paper_link'):
                content += f"   - [ë…¼ë¬¸ ë§í¬]({paper['paper_link']})\n"
            if paper.get('abstract'):
                content += f"   - Abstract: {paper['abstract']}\n"
            content += "\n"
        
        # Front Matter + Content
        yaml_header = "---\n"
        for key, value in front_matter.items():
            yaml_header += f"{key}: {value}\n" if not isinstance(value, list) else f"{key}: {value}\n"
        yaml_header += "---\n\n"
        full_content = yaml_header + content
        
        filepath.write_text(full_content, encoding='utf-8')
        print(f"ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì €ì¥: {filename}")
        
        return str(filepath)
    
    def create_monthly_summary_post(self, summary: Dict, force_update: bool = False) -> Optional[str]:
        """
        ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            summary: ì›”ê°„ ìš”ì•½ ë°ì´í„°
            force_update: ê¸°ì¡´ íŒŒì¼ì´ ìˆì–´ë„ ì—…ë°ì´íŠ¸í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ í¬ìŠ¤íŠ¸ ìƒì„±í•˜ì§€ ì•ŠìŒ
        if summary['total_papers'] == 0:
            print("âš ï¸ ì›”ê°„ ìš”ì•½ì— ë…¼ë¬¸ì´ 0ê°œì…ë‹ˆë‹¤. í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return None
        
        year = summary['year']
        month = summary['month']
        date_str = f"{year}-{month:02d}"
        
        filename = f"{date_str}-01-monthly-papers-summary.md"
        filepath = self.posts_dir / filename
        
        # ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‚´ìš© ë¹„êµ
        if filepath.exists():
            try:
                existing_content = filepath.read_text(encoding='utf-8')
                # ê¸°ì¡´ íŒŒì¼ì—ì„œ ë…¼ë¬¸ URL ì¶”ì¶œ
                existing_urls = set(re.findall(r'https://huggingface\.co/papers/[^\s\)]+', existing_content))
                # ìƒˆë¡œ ìƒì„±í•  ë…¼ë¬¸ URL ëª©ë¡
                new_urls = {paper.get('url', '') for paper in summary['top_papers'] if paper.get('url')}
                
                # ë‚´ìš©ì´ ê°™ìœ¼ë©´ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŒ
                if existing_urls == new_urls and not force_update:
                    print(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì›”ê°„ ìš”ì•½: {filename} (ë‚´ìš© ë™ì¼, ì—…ë°ì´íŠ¸ ìŠ¤í‚µ)")
                    return None
                
                if existing_urls == new_urls and force_update:
                    print(f"ê¸°ì¡´ íŒŒì¼ ë‚´ìš©ê³¼ ë™ì¼: {filename} (ì—…ë°ì´íŠ¸ ìŠ¤í‚µ)")
                    return None
                
                print(f"ê¸°ì¡´ íŒŒì¼ ë‚´ìš©ê³¼ ë‹¤ë¦„: {filename} (ì—…ë°ì´íŠ¸)")
            except Exception as e:
                print(f"ê¸°ì¡´ íŒŒì¼ í™•ì¸ ì˜¤ë¥˜: {e}, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        
        if not filepath.exists():
            print(f"ìƒˆ ì›”ê°„ ìš”ì•½ ìƒì„±: {filename}")
        
        # Front Matter
        front_matter = {
            'title': f'Hugging Face Papers Monthly Summary - {year}ë…„ {month}ì›”',
            'date': f"{year}-{month:02d}-01 09:00:00 +0900",
            'categories': ['Daily Papers', 'ì›”ê°„'],
            'tags': ['huggingface', 'papers', 'monthly', 'ai', 'summary'],
            'author': 'lim4349'
        }
        
        # ë³¸ë¬¸ ìƒì„±
        content = f"# Hugging Face Papers ì›”ê°„ ìš”ì•½ - {year}ë…„ {month}ì›”\n\n## ğŸ“Š í†µê³„\n\n"
        content += f"- **ì´ ë…¼ë¬¸ ìˆ˜**: {summary['total_papers']}ê°œ\n"
        content += f"- **ì´ ì¢‹ì•„ìš” ìˆ˜**: {summary['total_likes']:,}\n"
        content += f"- **í‰ê·  ì¢‹ì•„ìš” ìˆ˜**: {summary['average_likes']:.2f}\n"
        content += f"- **ìˆ˜ì§‘ ì¼ìˆ˜**: {summary['days_crawled']}ì¼\n\n"
        content += "## ğŸ”¥ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë…¼ë¬¸ Top 20\n\n"
        
        if summary['top_papers']:
            for i, paper in enumerate(summary['top_papers'], 1):
                content += f"{i}. **{paper.get('title', 'Untitled')}** - ğŸ‘ {paper.get('likes', 0)}\n"
                if paper.get('institution'):
                    content += f"   - ê¸°ê´€: {paper['institution']}\n"
                content += f"   - [HF í˜ì´ì§€]({paper.get('url', '#')})\n"
                if paper.get('paper_link'):
                    content += f"   - [ë…¼ë¬¸ ë§í¬]({paper['paper_link']})\n"
                if paper.get('abstract'):
                    content += f"   - Abstract: {paper['abstract']}\n"
                content += "\n"
        else:
            content += "ì´ë²ˆ ë‹¬ì— ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.\n\n"
        
        if summary['top_tags']:
            content += "## ğŸ·ï¸ ì¸ê¸° íƒœê·¸ Top 10\n\n"
            for i, tag_info in enumerate(summary['top_tags'], 1):
                content += f"{i}. `{tag_info['tag']}` - {tag_info['count']}íšŒ\n"
            content += "\n"
        
        # Front Matter + Content
        yaml_header = "---\n"
        for key, value in front_matter.items():
            yaml_header += f"{key}: {value}\n" if not isinstance(value, list) else f"{key}: {value}\n"
        yaml_header += "---\n\n"
        full_content = yaml_header + content
        
        filepath.write_text(full_content, encoding='utf-8')
        print(f"ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì €ì¥: {filename}")
        
        return str(filepath)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 50)
    print("Hugging Face Daily Papers í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 50)
    
    crawler = HFDailyPapersCrawler()
    
    # ì˜¤ëŠ˜ ë‚ ì§œë¡œ í¬ë¡¤ë§ (UTC ê¸°ì¤€)
    now_utc = datetime.utcnow()
    now_kst = now_utc + timedelta(hours=9)
    target_date = now_utc
    
    print(f"\ní˜„ì¬ ì‹œê°„: UTC {now_utc.strftime('%Y-%m-%d %H:%M')}, KST {now_kst.strftime('%Y-%m-%d %H:%M')}")
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ: {target_date.strftime('%Y-%m-%d')}")
    
    # 0ë‹¨ê³„: ì£¼ë§ ì²´í¬
    is_weekend = crawler.is_weekend(target_date)
    if is_weekend:
        weekday_name = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼'][target_date.weekday()]
        print(f"\nâš ï¸ ì˜¤ëŠ˜ì€ {weekday_name}(ì£¼ë§)ì…ë‹ˆë‹¤.")
        print("   Hugging FaceëŠ” ì£¼ë§ì— ìƒˆë¡œìš´ ë…¼ë¬¸ì„ ì—…ë¡œë“œí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("   í¬ë¡¤ë§ì„ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        print("\n" + "=" * 50)
        print("í¬ë¡¤ë§ ì™„ë£Œ: ì£¼ë§ ìŠ¤í‚µ")
        print("=" * 50)
        return
    
    # 1ë‹¨ê³„: í¬ë¡¤ë§ ì‹œê°„ëŒ€ í™•ì¸
    is_morning_crawl = 1 <= now_utc.hour < 12  # UTC 01:00-12:00 (KST 10:00-21:00)
    is_evening_crawl = 13 <= now_utc.hour < 14  # UTC 13:00-14:00 (KST 22:00-23:00)
    
    # 2ë‹¨ê³„: ì˜¤ëŠ˜ ë‚ ì§œ í˜ì´ì§€ì— ë…¼ë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸
    print(f"\n[1ë‹¨ê³„] ì˜¤ëŠ˜ ë‚ ì§œ í˜ì´ì§€ í™•ì¸ ì¤‘: {target_date.strftime('%Y-%m-%d')}")
    has_papers = crawler.check_date_has_papers(target_date)
    
    # 3ë‹¨ê³„: ê¸°ì¡´ í¬ìŠ¤íŠ¸ íŒŒì¼ í™•ì¸ (ì˜¤í›„ í¬ë¡¤ë§ ì‹œ)
    existing_post = crawler.has_existing_post(target_date) if is_evening_crawl else False
    
    if not has_papers:
        if is_morning_crawl:
            print(f"âš ï¸ ì˜¤ëŠ˜ ë‚ ì§œ ({target_date.strftime('%Y-%m-%d')})ì— ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("   í˜ì´ì§€ê°€ ì•„ì§ ê°±ì‹ ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            print("   í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ì˜¤í›„ 10ì‹œì— ë‹¤ì‹œ í™•ì¸í•©ë‹ˆë‹¤.")
            print("\n" + "=" * 50)
            print("í¬ë¡¤ë§ ì™„ë£Œ: ë…¼ë¬¸ ì—†ìŒ (ìŠ¤í‚µ)")
            print("=" * 50)
            return
        else:
            # ì˜¤í›„ í¬ë¡¤ë§ì¸ë°ë„ ë…¼ë¬¸ì´ ì—†ê³ , ê¸°ì¡´ í¬ìŠ¤íŠ¸ë„ ì—†ìœ¼ë©´ ì™„ì „ ìŠ¤í‚µ
            if not existing_post:
                print(f"âš ï¸ ì˜¤ëŠ˜ ë‚ ì§œ ({target_date.strftime('%Y-%m-%d')})ì— ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                print("   ì˜¤ì „ì—ë„ í¬ë¡¤ë§ë˜ì§€ ì•Šì•˜ê³ , ë…¼ë¬¸ë„ ì—†ìŠµë‹ˆë‹¤.")
                print("   í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                print("\n" + "=" * 50)
                print("í¬ë¡¤ë§ ì™„ë£Œ: ë…¼ë¬¸ ì—†ìŒ (ìŠ¤í‚µ)")
                print("=" * 50)
                return
            else:
                # ì˜¤í›„ í¬ë¡¤ë§: ê¸°ì¡´ í¬ìŠ¤íŠ¸ëŠ” ìˆì§€ë§Œ í˜„ì¬ ë…¼ë¬¸ì´ ì—†ëŠ” ê²½ìš°
                # (ì¢‹ì•„ìš” ìˆ˜ ì—…ë°ì´íŠ¸ëŠ” ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ìŠ¤í‚µ)
                print(f"âš ï¸ ì˜¤ëŠ˜ ë‚ ì§œ ({target_date.strftime('%Y-%m-%d')})ì— ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                print("   ê¸°ì¡´ í¬ìŠ¤íŠ¸ê°€ ìˆì§€ë§Œ í˜„ì¬ ë…¼ë¬¸ì´ ì—†ì–´ ì—…ë°ì´íŠ¸í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
                print("   í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                print("\n" + "=" * 50)
                print("í¬ë¡¤ë§ ì™„ë£Œ: ì—…ë°ì´íŠ¸í•  ë‚´ìš© ì—†ìŒ (ìŠ¤í‚µ)")
                print("=" * 50)
                return
    
    print("âœ… ì˜¤ëŠ˜ ë‚ ì§œì— ë…¼ë¬¸ì´ ìˆìŠµë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # 3ë‹¨ê³„: ë…¼ë¬¸ í¬ë¡¤ë§
    papers = crawler.fetch_daily_papers(target_date)
    
    print(f"\nì´ {len(papers)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
    
    # ë…¼ë¬¸ì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if not papers:
        print("âš ï¸ ë…¼ë¬¸ì´ 0ê°œì…ë‹ˆë‹¤. í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        print("\n" + "=" * 50)
        print("í¬ë¡¤ë§ ì™„ë£Œ: 0ê°œì˜ ë…¼ë¬¸ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        print("=" * 50)
        return
    
    # ë…¼ë¬¸ì´ ìˆìœ¼ë©´ ì¶œë ¥
    print("ì¢‹ì•„ìš” ìˆœìœ„ (Top 10):")
    for i, paper in enumerate(papers[:10], 1):
        likes = paper.get('likes', 0)
        title = paper.get('title', 'Unknown')[:60]
        print(f"  {i}. ğŸ‘ {likes} - {title}")
    
    # 4ë‹¨ê³„: í¬ë¡¤ë§ ì‹œê°„ëŒ€ì— ë”°ë¼ ì—…ë°ì´íŠ¸ ë°©ì‹ ê²°ì •
    # ì˜¤ì „ í¬ë¡¤ë§(UTC 01:00, KST 10:00): ìƒˆ íŒŒì¼ ìƒì„± (ê¸°ì¡´ íŒŒì¼ê³¼ ë¹„êµí•˜ì—¬ ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ ì—…ë°ì´íŠ¸)
    # ì˜¤í›„ í¬ë¡¤ë§(UTC 13:00, KST 22:00): ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ì¢‹ì•„ìš” ìˆ˜ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ë®ì–´ì“°ê¸°
    if is_evening_crawl:
        if existing_post:
            print("\n[ì˜¤í›„ í¬ë¡¤ë§] ê¸°ì¡´ í¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤.")
            print("   ì¢‹ì•„ìš” ìˆ˜ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ íŒŒì¼ì„ ë®ì–´ì“°ê¸°í•©ë‹ˆë‹¤.")
            force_update = True
        else:
            print("\n[ì˜¤í›„ í¬ë¡¤ë§] ê¸°ì¡´ í¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("   ì˜¤ì „ í¬ë¡¤ë§ì´ ìŠ¤í‚µë˜ì—ˆì§€ë§Œ í˜ì´ì§€ê°€ ê°±ì‹ ë˜ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            # ê¸°ì¡´ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì¼ë°˜ ìƒì„± ëª¨ë“œ (ë‚´ìš© ë¹„êµ í›„ ìƒì„±)
            force_update = False
    else:
        print("\n[ì˜¤ì „ í¬ë¡¤ë§] ìƒˆ ë…¼ë¬¸ í¬ë¡¤ë§ ë° íŒŒì¼ ìƒì„±/ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.")
        force_update = False
    
    # ë…¼ë¬¸ì´ ìˆì„ ë•Œë§Œ ë°ì´í„° ì €ì¥ ë° í¬ìŠ¤íŠ¸ ìƒì„±
    crawler.save_daily_data(papers, target_date)
    post_path = crawler.create_daily_summary_post(papers, target_date, force_update=force_update)
    if post_path:
        if force_update:
            print(f"âœ… ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ì¢‹ì•„ìš” ìˆ˜ ë°˜ì˜): {post_path}")
        else:
            print(f"âœ… ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±/ì—…ë°ì´íŠ¸: {post_path}")
    else:
        print("â„¹ï¸ ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì—†ìŒ")
    
    # ì›”ê°„ ìš”ì•½ ìƒì„± (ì´ì „ ë‹¬) - ë§¤ì›” 1ì¼ì—ë§Œ ì‹¤í–‰
    # ì˜¤ëŠ˜ì´ ë§¤ì›” 1ì¼ì´ë©´ ì´ì „ ë‹¬ì˜ ì›”ê°„ ìš”ì•½ ìƒì„±
    is_first_of_month = target_date.day == 1
    
    if is_first_of_month and is_morning_crawl:
        try:
            # ì´ì „ ë‹¬ ê³„ì‚°
            if target_date.month == 1:
                # 1ì›”ì´ë©´ ì‘ë…„ 12ì›”
                prev_year = target_date.year - 1
                prev_month = 12
            else:
                prev_year = target_date.year
                prev_month = target_date.month - 1
            
            print(f"\n[ì›”ê°„ ìš”ì•½] ì´ì „ ë‹¬({prev_year}ë…„ {prev_month}ì›”) ì›”ê°„ ìš”ì•½ ìƒì„± ì¤‘...")
            summary = crawler.generate_monthly_summary(prev_year, prev_month)
            # ì›”ê°„ ìš”ì•½ì€ ë…¼ë¬¸ì´ ìˆì„ ë•Œë§Œ ìƒì„±
            if summary['total_papers'] > 0:
                monthly_post_path = crawler.create_monthly_summary_post(summary, force_update=False)
                if monthly_post_path:
                    print(f"âœ… ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±/ì—…ë°ì´íŠ¸: {monthly_post_path}")
                else:
                    print("â„¹ï¸ ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ ì—†ìŒ (ë‚´ìš© ë™ì¼ ë˜ëŠ” ì´ë¯¸ ì¡´ì¬)")
            else:
                print(f"âš ï¸ {prev_year}ë…„ {prev_month}ì›”ì— ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ì›”ê°„ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
    elif is_first_of_month:
        print(f"\n[ì›”ê°„ ìš”ì•½] ì˜¤ëŠ˜ì€ {target_date.day}ì¼ì´ì§€ë§Œ ì˜¤í›„ í¬ë¡¤ë§ì´ë¯€ë¡œ ì›”ê°„ ìš”ì•½ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    else:
        print(f"\n[ì›”ê°„ ìš”ì•½] ì˜¤ëŠ˜ì€ {target_date.day}ì¼ì´ë¯€ë¡œ ì›”ê°„ ìš”ì•½ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (ë§¤ì›” 1ì¼ì—ë§Œ ìƒì„±)")
    
    print("\n" + "=" * 50)
    print(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(papers)}ê°œì˜ ë…¼ë¬¸ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    print("=" * 50)


if __name__ == "__main__":
    main()
