#!/usr/bin/env python3
"""
Hugging Face Daily Papers í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
GitHub Actionsì—ì„œ ì‹¤í–‰í•˜ì—¬ ì¼ì¼ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ê³  Jekyll í¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import re
import json
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from collections import defaultdict
import time


class HFDailyPapersCrawler:
    """Hugging Face Daily Papers í¬ë¡¤ëŸ¬"""

    def __init__(self, posts_dir: str = "_posts", data_dir: str = "_data/papers"):
        self.posts_dir = Path(posts_dir)
        self.posts_dir.mkdir(exist_ok=True)

        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

        self.base_url = "https://huggingface.co"
        self.papers_url = "https://huggingface.co/papers"
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }

    def fetch_daily_papers(self, target_date: Optional[datetime] = None) -> List[Dict]:
        """íŠ¹ì • ë‚ ì§œì˜ ë…¼ë¬¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì¢‹ì•„ìš” ìˆ˜ í¬í•¨)"""
        if target_date is None:
            target_date = datetime.utcnow()

        papers = self._fetch_daily_from_web(target_date)

        # ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬ í›„ ìƒìœ„ 10ê°œ ì„ íƒ
        papers.sort(key=lambda x: x.get("likes", 0), reverse=True)
        top_papers = papers[:10]

        # ìƒìœ„ 10ê°œì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        enriched_papers = []
        for i, paper in enumerate(top_papers, 1):
            print(f"  [{i}/10] ìˆ˜ì§‘ ì¤‘: {paper.get('title', 'Unknown')[:50]}...")
            enriched = self._enrich_paper_details(paper)
            enriched_papers.append(enriched)
            time.sleep(1)

        enriched_papers.sort(key=lambda x: x.get("likes", 0), reverse=True)
        return enriched_papers

    def _fetch_daily_from_web(self, target_date: datetime) -> List[Dict]:
        """ì›¹ í˜ì´ì§€ì—ì„œ ì¼ì¼ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        papers = []
        date_str = target_date.strftime("%Y-%m-%d")
        url = f"{self.papers_url}/date/{date_str}"

        try:
            response = requests.get(url, timeout=30, headers=self.headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

            paper_containers = soup.find_all(
                "div", class_=re.compile(r"flex\s+w-full\s+gap-6", re.I)
            )
            for container in paper_containers:
                try:
                    heading = container.find("h3")
                    if not heading:
                        continue

                    title_link = heading.find("a")
                    if not title_link:
                        continue

                    title = title_link.get_text(strip=True)
                    paper_url = title_link.get("href", "")
                    if not paper_url:
                        continue

                    if not paper_url.startswith("http"):
                        paper_url = (
                            self.base_url + paper_url
                            if paper_url.startswith("/")
                            else f"{self.base_url}/papers/{paper_url}"
                        )

                    # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ
                    likes = self._extract_likes_from_container(container)

                    # ê¸°ê´€ ì •ë³´ ì¶”ì¶œ
                    institution = self._extract_institution(container, heading)

                    papers.append(
                        {
                            "title": title,
                            "url": paper_url,
                            "published": target_date.isoformat(),
                            "likes": likes,
                            "institution": institution,
                        }
                    )
                except Exception:
                    continue
        except Exception:
            pass

        return papers

    def _extract_likes_from_container(self, container) -> int:
        """ì»¨í…Œì´ë„ˆì—ì„œ ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ"""
        for elem in container.find_all("div"):
            classes = elem.get("class", [])
            if isinstance(classes, list):
                classes_str = " ".join(classes)
            else:
                classes_str = str(classes)

            if "leading-none" in classes_str:
                text = elem.get_text(strip=True)
                if text.isdigit():
                    num = int(text)
                    if 1 <= num <= 100000:
                        return num
        return 0

    def _extract_institution(self, container, heading) -> str:
        """ì»¨í…Œì´ë„ˆì—ì„œ ê¸°ê´€ ì •ë³´ ì¶”ì¶œ"""
        current = heading.next_sibling
        for _ in range(10):
            if not current:
                break
            if hasattr(current, "get_text"):
                text = current.get_text(strip=True)
                if text and 2 < len(text) < 200 and not re.match(r"^[\d\s\-]+$", text):
                    return text
            current = getattr(current, "next_sibling", None)
        return ""

    def _enrich_paper_details(self, paper: Dict) -> Dict:
        """ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        url = paper.get("url", "")
        if not url:
            return paper

        try:
            response = requests.get(url, timeout=30, headers=self.headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, "html.parser")

            # ì¢‹ì•„ìš” ìˆ˜ ì—…ë°ì´íŠ¸
            likes = self._extract_likes_from_detail_page(soup)
            if likes > paper.get("likes", 0):
                paper["likes"] = likes

            # ì œëª©
            if not paper.get("title"):
                h1 = soup.find("h1") or soup.find("title")
                if h1:
                    paper["title"] = h1.get_text(strip=True)

            # ë…¼ë¬¸ ë§í¬
            for link in soup.find_all("a", href=True):
                href = link.get("href", "")
                if re.search(r"arxiv\.org", href, re.I):
                    if not href.startswith("http"):
                        href = (
                            "https://arxiv.org" + href if href.startswith("/") else href
                        )
                    paper["paper_link"] = href
                    break
                elif re.search(r"\.pdf$", href, re.I) and not paper.get("paper_link"):
                    paper["paper_link"] = (
                        self.base_url + href if href.startswith("/") else href
                    )
                    break

            # ì½”ë“œ ë§í¬
            for link in soup.find_all("a", href=re.compile(r"github", re.I)):
                paper["code_link"] = link.get("href", "")
                break

            # Abstract ì¶”ì¶œ
            if not paper.get("abstract") or len(paper.get("abstract", "")) < 50:
                paper["abstract"] = self._extract_abstract(
                    soup, paper.get("paper_link", "")
                )

        except Exception:
            pass

        return paper

    def _extract_likes_from_detail_page(self, soup) -> int:
        """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ"""
        for elem in soup.find_all("div"):
            classes = elem.get("class", [])
            if isinstance(classes, list):
                classes_str = " ".join(classes)
            else:
                classes_str = str(classes)

            if "leading-none" in classes_str:
                text = elem.get_text(strip=True)
                if text.isdigit():
                    num = int(text)
                    if 1 <= num <= 100000:
                        return num
        return 0

    def _extract_abstract(self, soup, paper_link: str) -> str:
        """Abstract ì¶”ì¶œ"""
        abstract = ""

        # ë…¼ë¬¸ ë§í¬ì—ì„œ ì‹œë„
        if paper_link:
            try:
                paper_response = requests.get(
                    paper_link, timeout=30, headers=self.headers
                )
                paper_response.raise_for_status()
                paper_soup = BeautifulSoup(paper_response.content, "html.parser")

                abstract_elem = (
                    paper_soup.find("blockquote", class_="abstract")
                    or paper_soup.find("div", class_="abstract")
                    or paper_soup.find("span", class_="abstract-text")
                    or paper_soup.find(class_=re.compile(r"abstract", re.I))
                )

                if abstract_elem:
                    abstract_text = abstract_elem.get_text(separator=" ", strip=True)
                    abstract_text = re.sub(
                        r"^Abstract\s*:?\s*", "", abstract_text, flags=re.I
                    )
                    abstract_text = re.sub(
                        r"^arXiv\s*:?\s*\d+\.\d+\s*", "", abstract_text, flags=re.I
                    )
                    if len(abstract_text) >= 50:
                        abstract = abstract_text.strip()
            except Exception:
                pass

        # Hugging Face í˜ì´ì§€ì—ì„œ ì‹œë„
        if not abstract or len(abstract) < 50:
            for selector in ['div[class*="abstract"]', 'div[class*="summary"]']:
                for elem in soup.select(selector):
                    text = elem.get_text(separator=" ", strip=True)
                    if 100 <= len(text) <= 2000:
                        if not any(
                            phrase in text.lower()
                            for phrase in [
                                "join the discussion",
                                "subscribe",
                                "get trending papers",
                            ]
                        ):
                            abstract = text
                            break

            # main/articleì—ì„œ ê¸´ ë¬¸ë‹¨ ì°¾ê¸°
            if not abstract:
                for container in soup.find_all(["main", "article"]):
                    for p in container.find_all(["p", "div"], limit=10):
                        text = p.get_text(separator=" ", strip=True)
                        if 150 <= len(text) <= 2000 and len(text.split()) >= 20:
                            if not any(
                                phrase in text.lower()
                                for phrase in [
                                    "join the discussion",
                                    "subscribe",
                                    "get trending papers",
                                ]
                            ):
                                abstract = text
                                break
                    if abstract:
                        break

        return abstract

    def has_existing_post(self, target_date: datetime) -> bool:
        """íŠ¹ì • ë‚ ì§œì˜ í¬ìŠ¤íŠ¸ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        date_str = target_date.strftime("%Y-%m-%d")
        filename = f"{date_str}-daily-papers-summary.md"
        filepath = self.posts_dir / filename
        return filepath.exists()

    def save_daily_data(
        self, papers: List[Dict], target_date: Optional[datetime] = None
    ) -> Optional[str]:
        """ì¼ê°„ ë…¼ë¬¸ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        if not papers:
            return None

        if target_date is None:
            target_date = datetime.utcnow()

        date_str = target_date.strftime("%Y-%m-%d")
        filename = f"daily-{date_str}.json"
        filepath = self.data_dir / filename

        # ê¸°ì¡´ íŒŒì¼ ë¹„êµ
        if filepath.exists():
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)
                    existing_dict = {
                        (p.get("url", ""), p.get("likes", 0))
                        for p in existing_data.get("papers", [])
                    }
                    new_dict = {
                        (p.get("url", ""), p.get("likes", 0))
                        for p in papers
                        if p.get("url")
                    }

                    if existing_dict == new_dict:
                        print(f"ë°ì´í„° ë™ì¼: {filename} (ì €ì¥ ìŠ¤í‚µ)")
                        return None
            except Exception:
                pass

        data = {
            "date": date_str,
            "crawled_at": datetime.utcnow().isoformat(),
            "total_papers": len(papers),
            "papers": papers,
        }

        filepath.write_text(
            json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8"
        )
        print(f"ë°ì´í„° ì €ì¥: {filename} ({len(papers)}ê°œ ë…¼ë¬¸)")
        return str(filepath)

    def generate_monthly_summary(self, year: int, month: int) -> Dict:
        """ì›”ê°„ ìš”ì•½ ìƒì„±"""
        daily_data = []
        start_date = datetime(year, month, 1)
        end_date = (
            datetime(year, month + 1, 1) if month < 12 else datetime(year + 1, 1, 1)
        )

        for i in range((end_date - start_date).days):
            current_date = start_date + timedelta(days=i)
            date_str = current_date.strftime("%Y-%m-%d")
            daily_file = self.data_dir / f"daily-{date_str}.json"

            if daily_file.exists():
                try:
                    with open(daily_file, "r", encoding="utf-8") as f:
                        daily_data.append(json.load(f))
                except Exception:
                    pass

        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        all_papers = []
        seen_urls = set()
        for daily in daily_data:
            for paper in daily.get("papers", []):
                url = paper.get("url", "")
                if url and url not in seen_urls:
                    seen_urls.add(url)
                    all_papers.append(paper)

        all_papers.sort(key=lambda x: x.get("likes", 0), reverse=True)
        top_papers = all_papers[:20]

        # íƒœê·¸ í†µê³„
        tag_stats = defaultdict(int)
        for paper in all_papers:
            for tag in paper.get("tags", []):
                tag_stats[tag] += 1

        summary = {
            "year": year,
            "month": month,
            "generated_at": datetime.utcnow().isoformat(),
            "total_papers": len(all_papers),
            "total_likes": sum(p.get("likes", 0) for p in all_papers),
            "average_likes": round(
                sum(p.get("likes", 0) for p in all_papers) / len(all_papers), 2
            )
            if all_papers
            else 0,
            "days_crawled": len(daily_data),
            "top_papers": top_papers,
            "top_tags": [
                {"tag": tag, "count": count}
                for tag, count in sorted(
                    tag_stats.items(), key=lambda x: x[1], reverse=True
                )[:10]
            ],
        }

        if len(all_papers) == 0:
            return summary

        filename = f"monthly-{year}-{month:02d}.json"
        filepath = self.data_dir / filename

        # ê¸°ì¡´ íŒŒì¼ ë¹„êµ
        if filepath.exists():
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)
                    existing_urls = {
                        p.get("url", "") for p in existing_data.get("top_papers", [])
                    }
                    new_urls = {p.get("url", "") for p in top_papers if p.get("url")}

                    if existing_urls == new_urls:
                        print(f"ì›”ê°„ ìš”ì•½ ë™ì¼: {filename} (ì €ì¥ ìŠ¤í‚µ)")
                        return summary
            except Exception:
                pass

        filepath.write_text(
            json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8"
        )
        print(f"ì›”ê°„ ìš”ì•½ ìƒì„±: {filename}")
        return summary

    def create_daily_summary_post(
        self,
        papers: List[Dict],
        target_date: Optional[datetime] = None,
        force_update: bool = False,
    ) -> Optional[str]:
        """ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±"""
        if not papers:
            return None

        if target_date is None:
            target_date = datetime.utcnow()

        kst_date = target_date + timedelta(hours=9)
        post_date = kst_date.replace(hour=9, minute=15, second=0, microsecond=0)
        filename_date = target_date.strftime("%Y-%m-%d")
        filename = f"{filename_date}-daily-papers-summary.md"
        filepath = self.posts_dir / filename

        # ê¸°ì¡´ íŒŒì¼ í™•ì¸
        if filepath.exists() and not force_update:
            try:
                existing_content = filepath.read_text(encoding="utf-8")
                existing_urls = set(
                    re.findall(
                        r"https://huggingface\.co/papers/[^\s\)]+", existing_content
                    )
                )
                new_urls = {
                    paper.get("url", "") for paper in papers if paper.get("url")
                }

                if existing_urls == new_urls:
                    print(f"ê¸°ì¡´ í¬ìŠ¤íŠ¸ ë™ì¼: {filename} (ì—…ë°ì´íŠ¸ ìŠ¤í‚µ)")
                    return None
            except Exception:
                pass

        # Front Matter
        front_matter = f"""---
title: Hugging Face Daily Papers - {filename_date}
date: {post_date.strftime("%Y-%m-%d %H:%M:%S")} +0900
categories: [Daily Papers, ì¼ê°„]
tags: [huggingface, papers, daily, ai]
author: lim4349
---

"""

        content = f"# Hugging Face Daily Papers - {filename_date}\n\n"
        content += (
            f"ì´ **{len(papers)}ê°œ**ì˜ ë…¼ë¬¸ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n## ğŸ“Š ì¢‹ì•„ìš” ìˆœìœ„\n\n"
        )

        for i, paper in enumerate(papers, 1):
            content += f"{i}. **{paper.get('title', 'Untitled')}** - ğŸ‘ {paper.get('likes', 0)}\n"
            if paper.get("institution"):
                content += f"   - ê¸°ê´€: {paper['institution']}\n"
            content += f"   - [HF í˜ì´ì§€]({paper.get('url', '#')})\n"
            if paper.get("paper_link"):
                content += f"   - [ë…¼ë¬¸ ë§í¬]({paper['paper_link']})\n"
            if paper.get("abstract"):
                content += f"   - Abstract: {paper['abstract']}\n"
            content += "\n"

        filepath.write_text(front_matter + content, encoding="utf-8")
        print(f"í¬ìŠ¤íŠ¸ ì €ì¥: {filename}")
        return str(filepath)

    def create_monthly_summary_post(
        self, summary: Dict, force_update: bool = False
    ) -> Optional[str]:
        """ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±"""
        if summary["total_papers"] == 0:
            return None

        year, month = summary["year"], summary["month"]
        date_str = f"{year}-{month:02d}"
        filename = f"{date_str}-01-monthly-papers-summary.md"
        filepath = self.posts_dir / filename

        if filepath.exists() and not force_update:
            try:
                existing_content = filepath.read_text(encoding="utf-8")
                existing_urls = set(
                    re.findall(
                        r"https://huggingface\.co/papers/[^\s\)]+", existing_content
                    )
                )
                new_urls = {
                    paper.get("url", "")
                    for paper in summary["top_papers"]
                    if paper.get("url")
                }

                if existing_urls == new_urls:
                    print(f"ê¸°ì¡´ ì›”ê°„ í¬ìŠ¤íŠ¸ ë™ì¼: {filename} (ì—…ë°ì´íŠ¸ ìŠ¤í‚µ)")
                    return None
            except Exception:
                pass

        front_matter = f"""---
title: Hugging Face Papers Monthly Summary - {year}ë…„ {month}ì›”
date: {year}-{month:02d}-01 09:00:00 +0900
categories: [Daily Papers, ì›”ê°„]
tags: [huggingface, papers, monthly, ai, summary]
author: lim4349
---

"""

        content = (
            f"# Hugging Face Papers ì›”ê°„ ìš”ì•½ - {year}ë…„ {month}ì›”\n\n## ğŸ“Š í†µê³„\n\n"
        )
        content += f"- **ì´ ë…¼ë¬¸ ìˆ˜**: {summary['total_papers']}ê°œ\n"
        content += f"- **ì´ ì¢‹ì•„ìš” ìˆ˜**: {summary['total_likes']:,}\n"
        content += f"- **í‰ê·  ì¢‹ì•„ìš” ìˆ˜**: {summary['average_likes']:.2f}\n"
        content += f"- **ìˆ˜ì§‘ ì¼ìˆ˜**: {summary['days_crawled']}ì¼\n\n"
        content += "## ğŸ”¥ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë…¼ë¬¸ Top 20\n\n"

        for i, paper in enumerate(summary["top_papers"], 1):
            content += f"{i}. **{paper.get('title', 'Untitled')}** - ğŸ‘ {paper.get('likes', 0)}\n"
            if paper.get("institution"):
                content += f"   - ê¸°ê´€: {paper['institution']}\n"
            content += f"   - [HF í˜ì´ì§€]({paper.get('url', '#')})\n"
            if paper.get("paper_link"):
                content += f"   - [ë…¼ë¬¸ ë§í¬]({paper['paper_link']})\n"
            if paper.get("abstract"):
                content += f"   - Abstract: {paper['abstract']}\n"
            content += "\n"

        if summary["top_tags"]:
            content += "## ğŸ·ï¸ ì¸ê¸° íƒœê·¸ Top 10\n\n"
            for i, tag_info in enumerate(summary["top_tags"], 1):
                content += f"{i}. `{tag_info['tag']}` - {tag_info['count']}íšŒ\n"
            content += "\n"

        filepath.write_text(front_matter + content, encoding="utf-8")
        print(f"ì›”ê°„ í¬ìŠ¤íŠ¸ ì €ì¥: {filename}")
        return str(filepath)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 50)
    print("Hugging Face Daily Papers í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 50)

    crawler = HFDailyPapersCrawler()

    now_utc = datetime.utcnow()
    now_kst = now_utc + timedelta(hours=9)
    target_date = now_utc

    print(
        f"\ní˜„ì¬ ì‹œê°„: UTC {now_utc.strftime('%Y-%m-%d %H:%M')}, KST {now_kst.strftime('%Y-%m-%d %H:%M')}"
    )
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ: {target_date.strftime('%Y-%m-%d')}")

    # ë…¼ë¬¸ í¬ë¡¤ë§
    papers = crawler.fetch_daily_papers(target_date)

    if not papers:
        print("ë…¼ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return

    print(f"\nì´ {len(papers)}ê°œ ë…¼ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ")

    # ë°ì´í„° ì €ì¥ ë° í¬ìŠ¤íŠ¸ ìƒì„±
    is_evening_crawl = 14 <= now_utc.hour < 15
    force_update = is_evening_crawl and crawler.has_existing_post(target_date)

    crawler.save_daily_data(papers, target_date)
    post_path = crawler.create_daily_summary_post(
        papers, target_date, force_update=force_update
    )

    if post_path:
        print(f"âœ… í¬ìŠ¤íŠ¸ ìƒì„±: {post_path}")

    # ì›”ê°„ ìš”ì•½ (ë§¤ì›” 1ì¼ ì˜¤ì „)
    if target_date.day == 1 and 2 <= now_utc.hour < 3:
        prev_month = target_date.replace(day=1) - timedelta(days=1)
        print(f"\nì›”ê°„ ìš”ì•½ ìƒì„±: {prev_month.year}ë…„ {prev_month.month}ì›”")
        summary = crawler.generate_monthly_summary(prev_month.year, prev_month.month)
        if summary["total_papers"] > 0:
            crawler.create_monthly_summary_post(summary)

    print("\n" + "=" * 50)
    print(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(papers)}ê°œ ë…¼ë¬¸ ì²˜ë¦¬")
    print("=" * 50)


if __name__ == "__main__":
    main()
