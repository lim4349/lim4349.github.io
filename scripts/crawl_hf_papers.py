#!/usr/bin/env python3
"""
Hugging Face Daily Papers í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
GitHub Actionsì—ì„œ ì‹¤í–‰í•˜ì—¬ ì¼ì¼ ë…¼ë¬¸ì„ ìˆ˜ì§‘í•˜ê³  Jekyll í¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
ì¼ê°„ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥í•˜ê³ , ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬í•˜ë©°, Abstractë„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import re
import json
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from collections import defaultdict
import time
try:
    import feedparser
except ImportError:
    feedparser = None


class HFDailyPapersCrawler:
    """Hugging Face Daily Papers í¬ë¡¤ëŸ¬"""
    
    def __init__(self, posts_dir: str = "_posts", data_dir: str = "_data/papers"):
        self.posts_dir = Path(posts_dir)
        self.posts_dir.mkdir(exist_ok=True)
        
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # Hugging Face Daily Papers ê´€ë ¨ URLë“¤
        self.base_url = "https://huggingface.co"
        self.papers_url = "https://huggingface.co/papers"
        # ì—¬ëŸ¬ RSS í”¼ë“œ URL ì‹œë„
        self.rss_urls = [
            "https://huggingface.co/blog/tags/papers/rss.xml",
            "https://huggingface.co/blog/rss.xml",  # ì „ì²´ ë¸”ë¡œê·¸ RSS
        ]
        self.rss_url = self.rss_urls[0]  # ê¸°ë³¸ê°’
    
    def fetch_daily_papers(self, target_date: Optional[datetime] = None) -> List[Dict]:
        """
        íŠ¹ì • ë‚ ì§œì˜ ë…¼ë¬¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤ (ì¢‹ì•„ìš” ìˆ˜ í¬í•¨).
        
        Args:
            target_date: í¬ë¡¤ë§í•  ë‚ ì§œ (Noneì´ë©´ ì˜¤ëŠ˜)
            
        Returns:
            ë…¼ë¬¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ (ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬ë¨)
        """
        if target_date is None:
            target_date = datetime.utcnow()
        
        papers = []
        
        try:
            # RSS í”¼ë“œì—ì„œ ì˜¤ëŠ˜ ë‚ ì§œì˜ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°
            if feedparser is not None:
                rss_papers = self._fetch_from_rss(target_date)
                papers.extend(rss_papers)
                print(f"RSS í”¼ë“œì—ì„œ {len(rss_papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")
            else:
                print("âš ï¸ feedparserê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. RSS í”¼ë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        # ì›¹ í˜ì´ì§€ì—ì„œë„ ì‹œë„
        try:
            web_papers = self._fetch_daily_from_web(target_date)
            papers.extend(web_papers)
            print(f"ì›¹ ìŠ¤í¬ë˜í•‘ì—ì„œ {len(web_papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")
        except Exception as e:
            print(f"ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        # ê° ë…¼ë¬¸ì˜ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (Abstract í¬í•¨)
        enriched_papers = []
        for paper in papers:
            try:
                enriched = self._enrich_paper_details(paper)
                enriched_papers.append(enriched)
                # ìš”ì²­ ê°„ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(1)
            except Exception as e:
                print(f"ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ ({paper.get('url', 'unknown')}): {e}")
                enriched_papers.append(paper)
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_papers = []
        for paper in enriched_papers:
            url = paper.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_papers.append(paper)
        
        # ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
        unique_papers.sort(key=lambda x: x.get('likes', 0), reverse=True)
        
        return unique_papers
    
    def _fetch_from_rss(self, target_date: datetime) -> List[Dict]:
        """RSS í”¼ë“œì—ì„œ íŠ¹ì • ë‚ ì§œì˜ ë…¼ë¬¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        if feedparser is None:
            return []
        
        papers = []
        feed = None
        
        # ì—¬ëŸ¬ RSS URL ì‹œë„
        for rss_url in self.rss_urls:
            try:
                print(f"RSS í”¼ë“œ ê°€ì ¸ì˜¤ê¸° ì‹œë„: {rss_url}")
                feed = feedparser.parse(rss_url)
                
                if not feed.entries:
                    print(f"âš ï¸ RSS í”¼ë“œì— í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤: {rss_url}")
                    print(f"   í”¼ë“œ ìƒíƒœ: {feed.get('status', 'unknown')}")
                    continue
                
                print(f"âœ… RSS í”¼ë“œì—ì„œ ì´ {len(feed.entries)}ê°œ í•­ëª© ë°œê²¬: {rss_url}")
                break  # ì„±ê³µí•œ RSS í”¼ë“œ ì‚¬ìš©
                
            except Exception as e:
                print(f"âš ï¸ RSS í”¼ë“œ ì‹¤íŒ¨ ({rss_url}): {e}")
                continue
        
        # feedê°€ ì—†ìœ¼ë©´ ë°˜í™˜
        if not feed or not feed.entries:
            print("âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ RSS í”¼ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return papers
        
        target_date_str = target_date.strftime('%Y-%m-%d')
        yesterday = (target_date - timedelta(days=1)).strftime('%Y-%m-%d')
        
        # ìµœì‹  í•­ëª©ë“¤ í™•ì¸ (ë‚ ì§œ ë§¤ì¹­ì´ ì‹¤íŒ¨í•´ë„ ìµœì‹  í•­ëª© ì‚¬ìš©)
        checked_count = 0
        for entry in feed.entries[:30]:  # ìµœì‹  30ê°œ í™•ì¸
            checked_count += 1
            entry_date_str = entry.get('published', '')
            
            # ë‚ ì§œ ë§¤ì¹­ (ì˜¤ëŠ˜ ë˜ëŠ” ì–´ì œ)
            if (target_date_str in entry_date_str or 
                yesterday in entry_date_str or 
                self._is_same_date(entry_date_str, target_date) or
                self._is_same_date(entry_date_str, target_date - timedelta(days=1))):
                paper = {
                    'title': entry.get('title', ''),
                    'url': entry.get('link', ''),
                    'published': entry.get('published', ''),
                    'summary': entry.get('summary', ''),
                    'likes': 0  # RSSì—ì„œëŠ” ì¢‹ì•„ìš” ìˆ˜ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ
                }
                papers.append(paper)
                print(f"  - ë§¤ì¹­ëœ ë…¼ë¬¸: {paper['title'][:50]}...")
        
        # ë‚ ì§œ ë§¤ì¹­ì´ ì‹¤íŒ¨í–ˆìœ¼ë©´ ìµœì‹  10ê°œ ì‚¬ìš©
        if not papers and checked_count > 0:
            print("âš ï¸ ë‚ ì§œ ë§¤ì¹­ ì‹¤íŒ¨, ìµœì‹  í•­ëª© ì‚¬ìš©")
            for entry in feed.entries[:10]:
                paper = {
                    'title': entry.get('title', ''),
                    'url': entry.get('link', ''),
                    'published': entry.get('published', datetime.utcnow().isoformat()),
                    'summary': entry.get('summary', ''),
                    'likes': 0
                }
                papers.append(paper)
        
        return papers
    
    def _fetch_daily_from_web(self, target_date: datetime) -> List[Dict]:
        """ì›¹ í˜ì´ì§€ì—ì„œ ì¼ì¼ ë…¼ë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        papers = []
        
        # ì˜¬ë°”ë¥¸ URL í˜•ì‹: /papers/date/YYYY-MM-DD
        date_str = target_date.strftime('%Y-%m-%d')
        url = f"{self.papers_url}/date/{date_str}"
        
        print(f"ì›¹ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œë„: {url}")
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, timeout=30, headers=headers)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë…¼ë¬¸ í•­ëª© ì°¾ê¸° (h3 íƒœê·¸ë¡œ ì œëª© ì°¾ê¸°)
            # êµ¬ì¡°: h3 > a (ì œëª© ë§í¬), ì¢‹ì•„ìš” ìˆ˜ëŠ” ê°™ì€ ë ˆë²¨ì— ìˆìŒ
            paper_headings = soup.find_all('h3')
            
            if not paper_headings:
                print("âš ï¸ h3 íƒœê·¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡° í™•ì¸ í•„ìš”")
                # ëŒ€ì•ˆ: ëª¨ë“  ë§í¬ì—ì„œ papers ë§í¬ ì°¾ê¸°
                all_links = soup.find_all('a', href=re.compile(r'/papers/[^/]+$'))
                print(f"  ëŒ€ì•ˆ: {len(all_links)}ê°œ papers ë§í¬ ë°œê²¬")
            
            for heading in paper_headings:
                try:
                    # ì œëª© ë§í¬ ì°¾ê¸°
                    title_link = heading.find('a')
                    if not title_link:
                        continue
                    
                    title = title_link.get_text(strip=True)
                    if not title:
                        continue
                    
                    paper_url = title_link.get('href', '')
                    
                    if not paper_url.startswith('http'):
                        if paper_url.startswith('/'):
                            paper_url = self.base_url + paper_url
                        else:
                            paper_url = f"{self.base_url}/papers/{paper_url}"
                    
                    # ì¢‹ì•„ìš” ìˆ˜ ì°¾ê¸° - h3ì˜ ë¶€ëª¨ ìš”ì†Œì—ì„œ ì°¾ê¸°
                    likes = 0
                    parent = heading.parent
                    if parent:
                        # ë¶€ëª¨ ìš”ì†Œì˜ í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì ì°¾ê¸°
                        parent_text = parent.get_text()
                        # ì¢‹ì•„ìš” ìˆ˜ëŠ” ë³´í†µ í° ìˆ«ìì´ê³ , h3 ê·¼ì²˜ì— ìˆìŒ
                        # í˜•ì‹: "97\n" ë˜ëŠ” " 97 " ê°™ì€ íŒ¨í„´
                        numbers = re.findall(r'\b(\d+)\b', parent_text)
                        
                        # h3 ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ìˆ«ìê°€ ì¢‹ì•„ìš” ìˆ˜ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                        # ë˜ëŠ” h3 ì•ì— ìˆëŠ” í° ìˆ«ì
                        if numbers:
                            # ì²« ë²ˆì§¸ í° ìˆ«ì (10 ì´ìƒ)ë¥¼ ì¢‹ì•„ìš” ìˆ˜ë¡œ ì¶”ì •
                            for num_str in numbers:
                                try:
                                    num = int(num_str)
                                    if num >= 10:  # ì¢‹ì•„ìš”ëŠ” ë³´í†µ 10 ì´ìƒ
                                        likes = num
                                        break
                                except (ValueError, TypeError):
                                    pass
                        
                        # h3ì˜ ë‹¤ìŒ í˜•ì œ ìš”ì†Œ í™•ì¸
                        if likes == 0:
                            current = heading.next_sibling
                            checked = 0
                            while current and checked < 3:
                                if hasattr(current, 'get_text'):
                                    text = current.get_text(strip=True)
                                    like_match = re.search(r'^(\d+)$', text)
                                    if like_match:
                                        num = int(like_match.group(1))
                                        if num >= 10:
                                            likes = num
                                            break
                                current = getattr(current, 'next_sibling', None)
                                checked += 1
                    
                    if title and paper_url:
                        papers.append({
                            'title': title,
                            'url': paper_url,
                            'published': target_date.isoformat(),
                            'likes': likes
                        })
                        print(f"  - ë°œê²¬: {title[:50]}... (ğŸ‘ {likes})")
                        
                except Exception as e:
                    print(f"  ë…¼ë¬¸ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
            
            print(f"ì›¹ í˜ì´ì§€ì—ì„œ {len(papers)}ê°œ ë…¼ë¬¸ ë°œê²¬")
                    
        except Exception as e:
            print(f"ì›¹ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
        
        return papers
    
    def _enrich_paper_details(self, paper: Dict) -> Dict:
        """ë…¼ë¬¸ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (Abstract í¬í•¨)"""
        url = paper.get('url', '')
        if not url:
            return paper
        
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Abstract/Description ì¶”ì¶œ
            abstract = paper.get('abstract', '')
            
            # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ Abstract ì°¾ê¸°
            abstract_selectors = [
                ('div', {'class': re.compile(r'abstract|description|summary', re.I)}),
                ('p', {'class': re.compile(r'abstract|description|summary', re.I)}),
                ('section', {'class': re.compile(r'abstract|description|summary', re.I)}),
            ]
            
            for tag, attrs in abstract_selectors:
                elem = soup.find(tag, attrs)
                if elem:
                    abstract = elem.get_text(strip=True)
                    if abstract:
                        break
            
            # ë©”íƒ€ ì„¤ëª…ë„ ì‹œë„
            if not abstract:
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                if meta_desc:
                    abstract = meta_desc.get('content', '')
            
            # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ (ìƒì„¸ í˜ì´ì§€ì—ì„œ)
            likes = paper.get('likes', 0)
            like_elements = soup.find_all(['span', 'div', 'button'], class_=re.compile(r'like|favorite|star', re.I))
            for elem in like_elements:
                like_text = elem.get_text(strip=True)
                like_match = re.search(r'(\d+)', like_text)
                if like_match:
                    likes = max(likes, int(like_match.group(1)))
            
            # ì œëª© ì—…ë°ì´íŠ¸
            title = paper.get('title', '')
            if not title:
                title_elem = soup.find('h1') or soup.find('title')
                if title_elem:
                    title = title_elem.get_text(strip=True)
            
            # ë…¼ë¬¸ ë§í¬ (arXiv, PDF ë“±)
            paper_link = paper.get('paper_link', '')
            paper_links = soup.find_all('a', href=re.compile(r'(arxiv|pdf|doi)', re.I))
            if paper_links and not paper_link:
                paper_link = paper_links[0].get('href', '')
            
            # ì½”ë“œ ë§í¬
            code_link = paper.get('code_link', '')
            code_links = soup.find_all('a', href=re.compile(r'(github|gitlab)', re.I))
            if code_links and not code_link:
                code_link = code_links[0].get('href', '')
            
            # íƒœê·¸
            tags = paper.get('tags', [])
            tag_elements = soup.find_all(['a', 'span'], class_=re.compile(r'tag|label|badge', re.I))
            if not tags:
                tags = [tag.get_text(strip=True) for tag in tag_elements[:10]]
            
            # ì—…ë°ì´íŠ¸ëœ ì •ë³´ ë°˜í™˜
            paper.update({
                'title': title,
                'abstract': abstract,
                'likes': likes,
                'paper_link': paper_link,
                'code_link': code_link,
                'tags': tags,
                'description': abstract[:500] if abstract else ''  # ìš”ì•½
            })
            
        except Exception as e:
            print(f"ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
        
        return paper
    
    def _is_same_date(self, date_str: str, target_date: datetime) -> bool:
        """ë‚ ì§œ ë¬¸ìì—´ì´ target_dateì™€ ê°™ì€ ë‚ ì¸ì§€ í™•ì¸"""
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒŒì‹±
            for fmt in ['%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%d %H:%M:%S', '%Y-%m-%d']:
                try:
                    parsed = datetime.strptime(date_str[:19], fmt)
                    return parsed.date() == target_date.date()
                except (ValueError, TypeError):
                    continue
        except (ValueError, TypeError):
            pass
        return False
    
    def save_daily_data(self, papers: List[Dict], target_date: Optional[datetime] = None) -> str:
        """
        ì¼ê°„ ë…¼ë¬¸ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
        """
        if target_date is None:
            target_date = datetime.utcnow()
        
        date_str = target_date.strftime('%Y-%m-%d')
        filename = f"daily-{date_str}.json"
        filepath = self.data_dir / filename
        
        data = {
            'date': date_str,
            'crawled_at': datetime.utcnow().isoformat(),
            'total_papers': len(papers),
            'papers': papers
        }
        
        filepath.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
        print(f"ì¼ê°„ ë°ì´í„° ì €ì¥: {filename} ({len(papers)}ê°œ ë…¼ë¬¸)")
        
        return str(filepath)
    
    def generate_monthly_summary(self, year: int, month: int) -> Dict:
        """
        ì›”ê°„ ìš”ì•½ ìƒì„±
        
        Args:
            year: ì—°ë„
            month: ì›” (1-12)
            
        Returns:
            ì›”ê°„ ìš”ì•½ ë°ì´í„°
        """
        # í•´ë‹¹ ì›”ì˜ ëª¨ë“  ì¼ê°„ ë°ì´í„° ë¡œë“œ
        daily_data = []
        start_date = datetime(year, month, 1)
        end_date = datetime(year, month + 1, 1) if month < 12 else datetime(year + 1, 1, 1)
        
        current_date = start_date
        while current_date < end_date:
            date_str = current_date.strftime('%Y-%m-%d')
            daily_file = self.data_dir / f"daily-{date_str}.json"
            
            if daily_file.exists():
                try:
                    with open(daily_file, 'r', encoding='utf-8') as f:
                        daily = json.load(f)
                        daily_data.append(daily)
                except Exception as e:
                    print(f"ì¼ê°„ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({date_str}): {e}")
            
            current_date += timedelta(days=1)
        
        # ëª¨ë“  ë…¼ë¬¸ ìˆ˜ì§‘
        all_papers = []
        paper_ids = set()
        
        for daily in daily_data:
            for paper in daily.get('papers', []):
                paper_id = paper.get('url', '') or paper.get('title', '')
                if paper_id and paper_id not in paper_ids:
                    paper_ids.add(paper_id)
                    all_papers.append(paper)
        
        # ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬
        all_papers.sort(key=lambda x: x.get('likes', 0), reverse=True)
        
        # í†µê³„
        total_papers = len(all_papers)
        total_likes = sum(p.get('likes', 0) for p in all_papers)
        top_papers = all_papers[:10]  # Top 10
        
        # íƒœê·¸ë³„ í†µê³„
        tag_stats = defaultdict(int)
        for paper in all_papers:
            for tag in paper.get('tags', []):
                tag_stats[tag] += 1
        
        top_tags = sorted(tag_stats.items(), key=lambda x: x[1], reverse=True)[:10]
        
        summary = {
            'year': year,
            'month': month,
            'generated_at': datetime.utcnow().isoformat(),
            'total_papers': total_papers,
            'total_likes': total_likes,
            'average_likes': round(total_likes / total_papers, 2) if total_papers > 0 else 0,
            'days_crawled': len(daily_data),
            'top_papers': top_papers,
            'top_tags': [{'tag': tag, 'count': count} for tag, count in top_tags],
            'daily_summaries': [
                {
                    'date': d.get('date'),
                    'total_papers': d.get('total_papers', 0)
                }
                for d in daily_data
            ]
        }
        
        # ì›”ê°„ ìš”ì•½ ì €ì¥
        filename = f"monthly-{year}-{month:02d}.json"
        filepath = self.data_dir / filename
        filepath.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding='utf-8')
        
        print(f"ì›”ê°„ ìš”ì•½ ìƒì„±: {filename}")
        return summary
    
    def create_daily_summary_post(self, papers: List[Dict], target_date: Optional[datetime] = None) -> Optional[str]:
        """
        ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        if target_date is None:
            target_date = datetime.utcnow()
        
        # íŒŒì¼ëª… ìƒì„±
        filename_date = target_date.strftime('%Y-%m-%d')
        filename = f"{filename_date}-daily-papers-summary.md"
        filepath = self.posts_dir / filename
        
        # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì¸ì§€ í™•ì¸
        if filepath.exists():
            print(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì¼ê°„ ìš”ì•½: {filename}")
            return None
        
        # Front Matter
        front_matter = {
            'title': f'Hugging Face Daily Papers - {filename_date}',
            'date': f"{target_date.strftime('%Y-%m-%d %H:%M:%S')} +0900",
            'categories': ['Daily Papers', 'ì¼ê°„'],
            'tags': ['huggingface', 'papers', 'daily', 'ai'],
            'author': 'lim4349'
        }
        
        # ë³¸ë¬¸ ìƒì„±
        content = f"# Hugging Face Daily Papers - {filename_date}\n\n"
        content += f"ì´ **{len(papers)}ê°œ**ì˜ ë…¼ë¬¸ì´ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
        
        # ì¢‹ì•„ìš” ìˆ˜ë³„ë¡œ ì •ë ¬ëœ ëª©ë¡
        content += "## ğŸ“Š ì¢‹ì•„ìš” ìˆœìœ„\n\n"
        
        for i, paper in enumerate(papers[:20], 1):  # Top 20
            likes = paper.get('likes', 0)
            title = paper.get('title', 'Untitled')
            url = paper.get('url', '#')
            
            content += f"{i}. **{title}** - ğŸ‘ {likes}\n"
            content += f"   - [HF í˜ì´ì§€]({url})\n"
            
            if paper.get('paper_link'):
                content += f"   - [ë…¼ë¬¸ ë§í¬]({paper['paper_link']})\n"
            
            if paper.get('abstract'):
                abstract = paper['abstract'][:200] + "..." if len(paper['abstract']) > 200 else paper['abstract']
                content += f"   - Abstract: {abstract}\n"
            
            content += "\n"
        
        # Front Matter + Content ì¡°í•©
        yaml_header = "---\n"
        for key, value in front_matter.items():
            if isinstance(value, list):
                yaml_header += f"{key}: {value}\n"
            else:
                yaml_header += f"{key}: {value}\n"
        yaml_header += "---\n\n"
        
        full_content = yaml_header + content
        
        filepath.write_text(full_content, encoding='utf-8')
        print(f"ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì €ì¥: {filename}")
        
        return str(filepath)
    
    def create_monthly_summary_post(self, summary: Dict) -> Optional[str]:
        """
        ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±
        
        Returns:
            ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” None
        """
        year = summary['year']
        month = summary['month']
        date_str = f"{year}-{month:02d}"
        
        filename = f"{date_str}-01-monthly-papers-summary.md"
        filepath = self.posts_dir / filename
        
        if filepath.exists():
            print(f"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì›”ê°„ ìš”ì•½: {filename}")
            return None
        
        # Front Matter
        front_matter = {
            'title': f'Hugging Face Papers Monthly Summary - {year}ë…„ {month}ì›”',
            'date': f"{year}-{month:02d}-01 09:00:00 +0900",
            'categories': ['Daily Papers', 'ì›”ê°„'],
            'tags': ['huggingface', 'papers', 'monthly', 'ai', 'summary'],
            'author': 'lim4349'
        }
        
        # ë³¸ë¬¸ ìƒì„±
        content = f"# Hugging Face Papers ì›”ê°„ ìš”ì•½ - {year}ë…„ {month}ì›”\n\n"
        content += "## ğŸ“Š í†µê³„\n\n"
        content += f"- **ì´ ë…¼ë¬¸ ìˆ˜**: {summary['total_papers']}ê°œ\n"
        content += f"- **ì´ ì¢‹ì•„ìš” ìˆ˜**: {summary['total_likes']:,}\n"
        content += f"- **í‰ê·  ì¢‹ì•„ìš” ìˆ˜**: {summary['average_likes']:.2f}\n"
        content += f"- **ìˆ˜ì§‘ ì¼ìˆ˜**: {summary['days_crawled']}ì¼\n\n"
        
        # Top Papers
        content += "## ğŸ”¥ ê°€ì¥ ì¸ê¸° ìˆëŠ” ë…¼ë¬¸ Top 10\n\n"
        for i, paper in enumerate(summary['top_papers'], 1):
            likes = paper.get('likes', 0)
            title = paper.get('title', 'Untitled')
            url = paper.get('url', '#')
            
            content += f"{i}. **{title}** - ğŸ‘ {likes}\n"
            content += f"   - [HF í˜ì´ì§€]({url})\n\n"
        
        # Top Tags
        if summary['top_tags']:
            content += "## ğŸ·ï¸ ì¸ê¸° íƒœê·¸ Top 10\n\n"
            for i, tag_info in enumerate(summary['top_tags'], 1):
                content += f"{i}. `{tag_info['tag']}` - {tag_info['count']}íšŒ\n"
            content += "\n"
        
        # YAML Header
        yaml_header = "---\n"
        for key, value in front_matter.items():
            if isinstance(value, list):
                yaml_header += f"{key}: {value}\n"
            else:
                yaml_header += f"{key}: {value}\n"
        yaml_header += "---\n\n"
        
        full_content = yaml_header + content
        
        filepath.write_text(full_content, encoding='utf-8')
        print(f"ì›”ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ì €ì¥: {filename}")
        
        return str(filepath)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 50)
    print("Hugging Face Daily Papers í¬ë¡¤ë§ ì‹œì‘")
    print("=" * 50)
    
    crawler = HFDailyPapersCrawler()
    
    # ì˜¤ëŠ˜ ë‚ ì§œì˜ ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°
    target_date = datetime.utcnow()
    print(f"\ní¬ë¡¤ë§ ëŒ€ìƒ ë‚ ì§œ: {target_date.strftime('%Y-%m-%d')}")
    
    papers = crawler.fetch_daily_papers(target_date)
    
    print(f"\nì´ {len(papers)}ê°œì˜ ë…¼ë¬¸ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\n")
    
    if papers:
        # ì¢‹ì•„ìš” ìˆ˜ë¡œ ì •ë ¬ëœ ëª©ë¡ ì¶œë ¥
        print("ì¢‹ì•„ìš” ìˆœìœ„ (Top 10):")
        for i, paper in enumerate(papers[:10], 1):
            likes = paper.get('likes', 0)
            title = paper.get('title', 'Unknown')[:60]
            print(f"  {i}. ğŸ‘ {likes} - {title}")
        
        # ì¼ê°„ ë°ì´í„° ì €ì¥
        crawler.save_daily_data(papers, target_date)
        
        # ì¼ê°„ ìš”ì•½ í¬ìŠ¤íŠ¸ ìƒì„±
        crawler.create_daily_summary_post(papers, target_date)
        
        # ì›”ê°„ ìš”ì•½ ìƒì„± (ì´ë²ˆ ë‹¬)
        # ë§¤ì¼ ì›”ê°„ ìš”ì•½ì„ ë‹¤ì‹œ ìƒì„±í•˜ì—¬ ìµœì‹  ë°ì´í„° ë°˜ì˜
        # (ë§¤ì›” 1ì¼ì—ë§Œ ìƒì„±í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ)
        current_year = target_date.year
        current_month = target_date.month
        
        # ë§¤ì›” 1ì¼ì—ë§Œ ì›”ê°„ ìš”ì•½ ìƒì„±í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ
        # if target_date.day == 1:
        summary = crawler.generate_monthly_summary(current_year, current_month)
        crawler.create_monthly_summary_post(summary)
    
    print("\n" + "=" * 50)
    print(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(papers)}ê°œì˜ ë…¼ë¬¸ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    print("=" * 50)


if __name__ == "__main__":
    main()
