---
title: Hugging Face Daily Papers - 2026-02-09
date: 2026-02-09 09:15:00 +0900
categories: [Daily Papers, ÏùºÍ∞Ñ]
tags: [huggingface, papers, daily, ai]
author: lim4349
---

# Hugging Face Daily Papers - 2026-02-09

Ï¥ù **10Í∞ú**Ïùò ÎÖºÎ¨∏Ïù¥ ÏàòÏßëÎêòÏóàÏäµÎãàÎã§.

## üìä Ï¢ãÏïÑÏöî ÏàúÏúÑ

1. **Baichuan-M3: Modeling Clinical Inquiry for Reliable Medical Decision-Making** - üëç 53
   - Í∏∞Í¥Ä: Baichuan Intelligent Technology1861
   - [HF ÌéòÏù¥ÏßÄ](https://huggingface.co/papers/2602.06570)
   - [ÎÖºÎ¨∏ ÎßÅÌÅ¨](https://arxiv.org/abs/2602.06570)
   - Abstract: We introduce Baichuan-M3, a medical-enhanced large language model engineered to shift the paradigm from passive question-answering to active, clinical-grade decision support. Addressing the limitations of existing systems in open-ended consultations, Baichuan-M3 utilizes a specialized training pipeline to model the systematic workflow of a physician. Key capabilities include: (i) proactive information acquisition to resolve ambiguity; (ii) long-horizon reasoning that unifies scattered evidence into coherent diagnoses; and (iii) adaptive hallucination suppression to ensure factual reliability. Empirical evaluations demonstrate that Baichuan-M3 achieves state-of-the-art results on HealthBench, the newly introduced HealthBench-Hallu and ScanBench, significantly outperforming GPT-5.2 in clinical inquiry, advisory and safety. The models are publicly available at this https URL .

2. **OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions** - üëç 50
   - Í∏∞Í¥Ä: ¬∑19 authors191
   - [HF ÌéòÏù¥ÏßÄ](https://huggingface.co/papers/2602.05843)
   - [ÎÖºÎ¨∏ ÎßÅÌÅ¨](https://arxiv.org/abs/2602.05843)
   - Abstract: The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at this https URL

3. **F-GRPO: Don't Let Your Policy Learn the Obvious and Forget the Rare** - üëç 48
   - Í∏∞Í¥Ä: T-Tech1
   - [HF ÌéòÏù¥ÏßÄ](https://huggingface.co/papers/2602.06717)
   - [ÎÖºÎ¨∏ ÎßÅÌÅ¨](https://arxiv.org/abs/2602.06717)
   - Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is commonly based on group sampling to estimate advantages and stabilize policy updates. In practice, large group sizes are not feasible due to computational limits, which biases learning toward trajectories that are already likely. Smaller groups often miss rare-correct trajectories while still containing mixed rewards, concentrating probability on common solutions. We derive the probability that updates miss rare-correct modes as a function of group size, showing non-monotonic behavior, and characterize how updates redistribute mass within the correct set, revealing that unsampled-correct mass can shrink even as total correct mass grows. Motivated by this analysis, we propose a difficulty-aware advantage scaling coefficient, inspired by Focal loss, that down-weights updates on high-success prompts. The lightweight modification can be directly integrated into any group-relative RLVR algorithm such as GRPO, DAPO, and CISPO. On Qwen2.5-7B across in-domain and out-of-domain benchmarks, our method improves pass@256 from 64.1 $\rightarrow$ 70.3 (GRPO), 69.3 $\rightarrow$ 72.5 (DAPO), and 73.2 $\rightarrow$ 76.8 (CISPO), while preserving or improving pass@1, without increasing group size or computational cost.

4. **On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models** - üëç 44
   - Í∏∞Í¥Ä: ¬∑7 authors5212
   - [HF ÌéòÏù¥ÏßÄ](https://huggingface.co/papers/2602.03392)
   - [ÎÖºÎ¨∏ ÎßÅÌÅ¨](https://arxiv.org/abs/2602.03392)
   - Abstract: Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.

5. **AudioSAE: Towards Understanding of Audio-Processing Models with Sparse AutoEncoders** - üëç 39
   - Í∏∞Í¥Ä: HUAWEI Noah's Ark Lab21
   - [HF ÌéòÏù¥ÏßÄ](https://huggingface.co/papers/2602.05027)
   - [ÎÖºÎ¨∏ ÎßÅÌÅ¨](https://arxiv.org/abs/2602.05027)
   - Abstract: Sparse Autoencoders (SAEs) are powerful tools for interpreting neural representations, yet their use in audio remains underexplored. We train SAEs across all encoder layers of Whisper and HuBERT, provide an extensive evaluation of their stability, interpretability, and show their practical utility. Over 50% of the features remain consistent across random seeds, and reconstruction quality is preserved. SAE features capture general acoustic and semantic information as well as specific events, including environmental noises and paralinguistic sounds (e.g. laughter, whispering) and disentangle them effectively, requiring removal of only 19-27% of features to erase a concept. Feature steering reduces Whisper's false speech detections by 70% with negligible WER increase, demonstrating real-world applicability. Finally, we find SAE features correlated with human EEG activity during speech perception, indicating alignment with human neural processing. The code and checkpoints are available at this https URL .

6. **MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration** - üëç 28
   - Í∏∞Í¥Ä: Microsoft1
   - [HF ÌéòÏù¥ÏßÄ](https://huggingface.co/papers/2602.01734)
   - [ÎÖºÎ¨∏ ÎßÅÌÅ¨](https://arxiv.org/abs/2602.01734)
   - Abstract: Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $\mu$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.

7. **DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos** - üëç 19
   - Í∏∞Í¥Ä: NVIDIA
   - [HF ÌéòÏù¥ÏßÄ](https://huggingface.co/papers/2602.06949)
   - [ÎÖºÎ¨∏ ÎßÅÌÅ¨](https://arxiv.org/abs/2602.06949)
   - Abstract: Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.

8. **Judging What We Cannot Solve: A Consequence-Based Approach for Oracle-Free Evaluation of Research-Level Math** - üëç 16
   - Í∏∞Í¥Ä: ¬∑8 authors1
   - [HF ÌéòÏù¥ÏßÄ](https://huggingface.co/papers/2602.06291)
   - [ÎÖºÎ¨∏ ÎßÅÌÅ¨](https://arxiv.org/abs/2602.06291)
   - Abstract: Recent progress in reasoning models suggests that generating plausible attempts for research-level mathematics may be within reach, but verification remains a bottleneck, consuming scarce expert time. We hypothesize that a meaningful solution should contain enough method-level information that, when applied to a neighborhood of related questions, it should yield better downstream performance than incorrect solutions. Building on this idea, we propose \textbf{Consequence-Based Utility}, an oracle-free evaluator that scores each candidate by testing its value as an in-context exemplar in solving related yet verifiable questions. Our approach is evaluated on an original set of research-level math problems, each paired with one expert-written solution and nine LLM-generated solutions. Notably, Consequence-Based Utility consistently outperforms reward models, generative reward models, and LLM judges on ranking quality. Specifically, for GPT-OSS-120B, it improves Acc@1 from 67.2 to 76.3 and AUC from 71.4 to 79.6, with similarly large AUC gains on GPT-OSS-20B (69.0 to 79.2). Furthermore, compared to LLM-Judges, it also exhibits a larger solver-evaluator gap, maintaining a stronger correct-wrong separation even on instances where the underlying solver often fails to solve.

9. **Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training** - üëç 16
   - Í∏∞Í¥Ä: ¬∑9 authors1
   - [HF ÌéòÏù¥ÏßÄ](https://huggingface.co/papers/2602.05940)
   - [ÎÖºÎ¨∏ ÎßÅÌÅ¨](https://arxiv.org/abs/2602.05940)
   - Abstract: Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.

10. **Self-Improving World Modelling with Latent Actions** - üëç 15
   - Í∏∞Í¥Ä: ¬∑7 authors1
   - [HF ÌéòÏù¥ÏßÄ](https://huggingface.co/papers/2602.06130)
   - [ÎÖºÎ¨∏ ÎßÅÌÅ¨](https://arxiv.org/abs/2602.06130)
   - Abstract: Internal modelling of the world -- predicting transitions between previous states $X$ and next states $Y$ under actions $Z$ -- is essential to reasoning and planning for LLMs and VLMs. Learning such models typically requires costly action-labelled trajectories. We propose SWIRL, a self-improvement framework that learns from state-only sequences by treating actions as a latent variable and alternating between Forward World Modelling (FWM) $P_\theta(Y|X,Z)$ and an Inverse Dynamics Modelling (IDM) $Q_\phi(Z|X,Y)$. SWIRL iterates two phases: (1) Variational Information Maximisation, which updates the FWM to generate next states that maximise conditional mutual information with latent actions given prior states, encouraging identifiable consistency; and (2) ELBO Maximisation, which updates the IDM to explain observed transitions, effectively performing coordinate ascent. Both models are trained with reinforcement learning (specifically, GRPO) with the opposite frozen model's log-probability as a reward signal. We provide theoretical learnability guarantees for both updates, and evaluate SWIRL on LLMs and VLMs across multiple environments: single-turn and multi-turn open-world visual dynamics and synthetic textual environments for physics, web, and tool calling. SWIRL achieves gains of 16% on AURORABench, 28% on ByteMorph, 16% on WorldPredictionBench, and 14% on StableToolBench.

