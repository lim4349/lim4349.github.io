---
title: h100 k3s 설정기 (docker 모드)
date: 2026-01-07 09:15:00 +0900
categories: ['운영']
tags: ['k3s', 'docker', 'traefik', 'rancher', 'jupyterhub', 'mlflow', 'gpu', 'h100']
author: lim4349
---

# h100 k3s 설정기 (docker 모드)

`h100` 노드에 k3s를 올리면서 남긴 운영 기록을 한 번에 모았다. 리소스 YAML/values는 별도 보관하며 게시 시에는 동봉하지 않는다. (레포 내 폴더명에 팀명이 섞여 있어도 여기서는 모두 h100으로 표기)

> 기본 구조: k3s **docker 모드 + Traefik**(기본 Ingress). 도메인은 `*.<bastion IP>.nip.io` 형태로 붙인다. `/dataset`는 NFS이므로 **docker/containerd data-root는 로컬 `/`**(200G 고정)로 유지해야 한다.

## 환경/정책 요약

- K3s: docker 모드, 단일 서버 `h100-80g-4`
- Docker 기본 런타임: `nvidia` (GPU 전용)
- 노드 라벨: `gpu=nvidia`, `environment=h100`
- Ingress 도메인: `*.<bastion IP>.nip.io`
- MIG 사용, device-plugin `MIG_STRATEGY=mixed`
- 주요 볼륨 경로: `/dataset/jupyterhub`, `/dataset/mlflow`, `/dataset/monitoring/*`, `/dataset/huggingface`

## 특이 환경 메모

- **바스티온 경유 접속**: 대상 서버는 Bastion을 통해 SSH 프록시로만 접근 가능. kubeconfig와 Helm repo 업데이트 시 프록시/내부 DNS를 확인.
- **루트 200G 고정**: `/` 용량이 200G라 런타임 data-root는 로컬(`/var/lib/docker`, `/var/lib/containerd`)에 유지. `/dataset`는 NFS라 overlayfs upperdir로 쓸 수 없어 이전 시도에서 `invalid argument`가 발생했다. 데이터/캐시는 `/dataset`로, 런타임은 `/`에 분리.

## 설치/구성 순서

### 1) Docker 런타임 (NVIDIA 기본값)

```bash
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json >/dev/null <<'EOF'
{
  "runtimes": {
    "nvidia": { "path": "nvidia-container-runtime", "runtimeArgs": [] }
  },
  "default-runtime": "nvidia"
}
EOF
sudo systemctl restart docker
```

### 2) k3s (docker 모드, 서버 노드명 고정, 루트 200G 고려)

```bash
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --docker --node-name h100-80g-4" sh -
```

### 3) kubeconfig 복사

```bash
mkdir -p ~/.kube
sudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config
sudo chown $USER:$USER ~/.kube/config
```

### 4) 노드 라벨

```bash
kubectl label node h100-80g-4 gpu=nvidia
kubectl label node h100-80g-4 environment=h100
```

### 5) GPU 리소스 + 모니터링

```bash
kubectl apply -f nvidia-runtime-class.yaml
kubectl apply -f nvidia-device-plugin.yml
kubectl apply -f dcgm-exporter.yaml
kubectl apply -f dcgm-servicemonitor.yaml
```

확인: `kubectl get pods -n kube-system | grep nvidia-device-plugin`, `kubectl get node h100-80g-4 -o jsonpath='{.status.allocatable}' | tr ' ' '\n' | grep -E 'nvidia.com/gpu|mig'`

### 6) Rancher + cert-manager

```bash
helm repo add jetstack https://charts.jetstack.io
helm repo update

kubectl create namespace cert-manager
helm upgrade --install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --version v1.14.4 \
  --set crds.enabled=true \
  --wait --timeout 10m

helm repo add rancher-latest https://releases.rancher.com/server-charts/latest
helm repo update
kubectl create namespace cattle-system --dry-run=client -o yaml | kubectl apply -f -
helm upgrade --install rancher rancher-latest/rancher \
  --namespace cattle-system \
  --set hostname=rancher.<bastion IP>.nip.io \
  --set replicas=1 \
  --values rancher-values.yaml \
  --wait --timeout 10m
```

### 7) Rancher Monitoring (Grafana/Prometheus)

```bash
kubectl create namespace cattle-monitoring-system
helm repo add rancher-charts https://charts.rancher.io
helm repo update
helm install rancher-monitoring-crd rancher-charts/rancher-monitoring-crd -n cattle-monitoring-system
helm install rancher-monitoring rancher-charts/rancher-monitoring -n cattle-monitoring-system
```

IPv6 미사용 환경이면 ConfigMap에서 `[::]` 리스닝을 제거(아래 트러블슈팅 8번 참고).

### 8) JupyterHub

```bash
kubectl create namespace jupyter --dry-run=client -o yaml | kubectl apply -f -
kubectl apply -f jupyterhub-hook-image-awaiter-rbac.yaml

helm repo add jupyterhub https://jupyterhub.github.io/helm-chart/
helm repo update
helm upgrade --install jupyterhub jupyterhub/jupyterhub \
  --namespace jupyter \
  --version 4.2.0 \
  --values jupyter-values.yaml
```

스토리지: `/dataset/jupyterhub` PV/PVC 사용 (`jupyter-pv-pvc.yaml`). 루트 200G 한도 때문에 런타임 data-root는 `/`에, 대용량 데이터는 `/dataset`에 분리.

### 9) MLflow

```bash
kubectl apply -f mlflow-pv-pvc.yaml
kubectl apply -f allow-from-jupyter.yaml

helm repo add mlflow-charts https://community-charts.github.io/helm-charts
helm repo update
helm upgrade --install mlflow mlflow-charts/mlflow \
  -n mlflow --create-namespace \
  -f mlflow-values.yaml
```

`Invalid Host header`가 나오면 values에 설정된 호스트와 nip.io 도메인이 일치하는지 확인.

### 10) Pushgateway / whoami (기본 점검용)

```bash
kubectl apply -f pushgateway.yaml
kubectl apply -f whoami.yaml
```

## 운영 체크

- 클러스터 상태: `kubectl get nodes`, `kubectl get pods -A`
- GPU/MIG 할당량: `kubectl get node h100-80g-4 -o jsonpath='{.status.allocatable}'`
- Ingress: `kubectl get ingress -A`
- NFS는 데이터 전용(`/dataset`), 런타임 data-root는 로컬(`/var/lib/docker`, `/var/lib/containerd`)

## Troubleshooting 요약 (TROUBLESHOOTING_LOG.md)

1) **kubectl TLS 오류 (x509 unknown authority)**  
`~/.kube/config`를 `/etc/rancher/k3s/k3s.yaml`로 교체.

2) **Docker/Containerd blob not found**  
`/var/lib/docker`, `/var/lib/containerd` 정리 후 재시작.

3) **overlayfs 오류 (NFS data-root에서 invalid argument)**  
런타임 data-root를 로컬 `/`로 유지, `/dataset`는 캐시/데이터 전용.

4) **디스크 부족 (no space left on device)**  
루트가 200G 고정이므로 빌드 캐시가 쌓이면 바로 포화된다. `docker system prune -af`, `docker builder prune -af`로 정리.

5) **JupyterHub hook-image-awaiter 403**  
RBAC 보강 YAML(`jupyterhub-hook-image-awaiter-rbac.yaml`) 적용.

6) **MLflow Invalid Host header**  
values의 호스트와 nip.io 도메인 일치, 필요 시 hosts+SSH 터널(`k3s/ingress/mlflow-tunnel.md`) 활용.

7) **Rancher 설치 시 Issuer CRD 없음**  
`cert-manager` CRD 먼저 적용: `https://github.com/cert-manager/cert-manager/releases/download/v1.14.4/cert-manager.crds.yaml`.

8) **Rancher Monitoring nginx IPv6 CrashLoop**  
Grafana/Prometheus nginx ConfigMap에서 `[::]` 리스닝 라인 삭제 후 rollout restart.

9) **Helm 캐시 오류 (broken .cache symlink)**  
`HELM_CACHE_HOME/CONFIG_HOME/DATA_HOME`를 `/dataset/helm/*`로 재지정 후 디렉터리 생성.

10) **완료된 Pod 정리**  
`kubectl delete pod -A --field-selector=status.phase=Succeeded --grace-period=0 --force`.

11) **svclb-proxy-public Pending**  
외부 LB가 없을 때 정상. 필요 시 서비스 타입을 NodePort로 변경.

## 참고 파일

업로드 시 레포 내 참고 YAML/values 파일은 제외한다. 필요한 매니페스트와 Helm values는 별도 안전한 위치에 보관한 뒤 위 명령의 파일명만 맞춰 사용한다.
